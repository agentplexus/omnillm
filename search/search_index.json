{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OmniLLM","text":"<p>Unified Go SDK for Large Language Models</p> <p> </p> <p>OmniLLM is a unified Go SDK that provides a consistent interface for interacting with multiple Large Language Model (LLM) providers including OpenAI, Anthropic (Claude), Google Gemini, X.AI (Grok), and Ollama. It implements the Chat Completions API pattern and offers both synchronous and streaming capabilities.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Multi-Provider Support: OpenAI, Anthropic (Claude), Google Gemini, X.AI (Grok), Ollama, plus external providers (AWS Bedrock, etc.)</li> <li>Unified API: Same interface across all providers</li> <li>Streaming Support: Real-time response streaming for all providers</li> <li>Conversation Memory: Persistent conversation history using Key-Value Stores</li> <li>Fallback Providers: Automatic failover to backup providers when primary fails</li> <li>Circuit Breaker: Prevent cascading failures by temporarily skipping unhealthy providers</li> <li>Token Estimation: Pre-flight token counting to validate requests before sending</li> <li>Response Caching: Cache identical requests with configurable TTL to reduce costs</li> <li>Observability Hooks: Extensible hooks for tracing, logging, and metrics</li> <li>Retry with Backoff: Automatic retries for transient failures (rate limits, 5xx errors)</li> <li>Tool Calling: Function/tool calling support for agentic workflows</li> <li>Type Safe: Full Go type safety with comprehensive error handling</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n\n    \"github.com/plexusone/omnillm\"\n)\n\nfunc main() {\n    client, err := omnillm.NewClient(omnillm.ClientConfig{\n        Providers: []omnillm.ProviderConfig{\n            {Provider: omnillm.ProviderNameOpenAI, APIKey: \"your-openai-api-key\"},\n        },\n    })\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer client.Close()\n\n    response, err := client.CreateChatCompletion(context.Background(), &amp;omnillm.ChatCompletionRequest{\n        Model: omnillm.ModelGPT4o,\n        Messages: []omnillm.Message{\n            {Role: omnillm.RoleUser, Content: \"Hello! How can you help me today?\"},\n        },\n    })\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    fmt.Printf(\"Response: %s\\n\", response.Choices[0].Message.Content)\n}\n</code></pre>"},{"location":"#supported-providers","title":"Supported Providers","text":"Provider Models Features OpenAI GPT-5, GPT-4.1, GPT-4o, GPT-4o-mini Chat, Streaming, Tool Calling Anthropic Claude-Opus-4.1, Claude-Sonnet-4, Claude-3.7-Sonnet Chat, Streaming, System messages Google Gemini Gemini-2.5-Pro, Gemini-2.5-Flash, Gemini-1.5-Pro Chat, Streaming X.AI Grok-4.1-Fast, Grok-4, Grok-3 Chat, Streaming, 2M context Ollama Llama 3, Mistral, CodeLlama Chat, Streaming, Local inference AWS Bedrock Claude models, Titan models Chat (external module)"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Installation - Get OmniLLM set up</li> <li>Quick Start - Your first LLM call</li> <li>Providers - Configure specific providers</li> <li>Features - Explore advanced features</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>OmniLLM uses a clean, modular architecture that separates concerns and enables easy extensibility.</p>"},{"location":"architecture/#directory-structure","title":"Directory Structure","text":"<pre><code>omnillm/\n\u251c\u2500\u2500 client.go            # Main ChatClient wrapper\n\u251c\u2500\u2500 providers.go         # Factory functions for built-in providers\n\u251c\u2500\u2500 types.go             # Type aliases for backward compatibility\n\u251c\u2500\u2500 memory.go            # Conversation memory management\n\u251c\u2500\u2500 observability.go     # ObservabilityHook interface\n\u251c\u2500\u2500 errors.go            # Unified error handling\n\u251c\u2500\u2500 *_test.go            # Comprehensive unit tests\n\u251c\u2500\u2500 provider/            # Public interface package for external providers\n\u2502   \u251c\u2500\u2500 interface.go     # Provider interface\n\u2502   \u2514\u2500\u2500 types.go         # Unified request/response types\n\u251c\u2500\u2500 providers/           # Individual provider packages\n\u2502   \u251c\u2500\u2500 openai/\n\u2502   \u2502   \u251c\u2500\u2500 openai.go    # HTTP client\n\u2502   \u2502   \u251c\u2500\u2500 types.go     # OpenAI-specific types\n\u2502   \u2502   \u251c\u2500\u2500 adapter.go   # provider.Provider implementation\n\u2502   \u2502   \u2514\u2500\u2500 *_test.go    # Provider tests\n\u2502   \u251c\u2500\u2500 anthropic/\n\u2502   \u251c\u2500\u2500 gemini/\n\u2502   \u251c\u2500\u2500 xai/\n\u2502   \u2514\u2500\u2500 ollama/\n\u2514\u2500\u2500 testing/             # Test utilities\n    \u2514\u2500\u2500 mock_kvs.go      # Mock KVS for memory testing\n</code></pre>"},{"location":"architecture/#key-architecture-benefits","title":"Key Architecture Benefits","text":"<ul> <li>Public Interface: The <code>provider</code> package exports the <code>Provider</code> interface that external packages can implement</li> <li>Reference Implementation: Internal providers follow the exact same structure that external providers should use</li> <li>Direct Injection: External providers are injected via <code>ClientConfig.CustomProvider</code> without modifying core code</li> <li>Modular Design: Each provider is self-contained with its own HTTP client, types, and adapter</li> <li>Testable: Clean interfaces that can be easily mocked and tested</li> <li>Extensible: New providers can be added without touching existing code</li> <li>Native Implementation: Uses standard <code>net/http</code> for direct API communication (no official SDK dependencies)</li> </ul>"},{"location":"architecture/#provider-interface","title":"Provider Interface","text":"<pre><code>type Provider interface {\n    // Name returns the provider name\n    Name() string\n\n    // CreateChatCompletion performs a synchronous chat completion\n    CreateChatCompletion(ctx context.Context, req *ChatCompletionRequest) (*ChatCompletionResponse, error)\n\n    // CreateChatCompletionStream performs a streaming chat completion\n    CreateChatCompletionStream(ctx context.Context, req *ChatCompletionRequest) (ChatCompletionStream, error)\n\n    // Close releases any resources\n    Close() error\n}\n</code></pre>"},{"location":"architecture/#requestresponse-flow","title":"Request/Response Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Client   \u2502\u2500\u2500\u2500\u25ba\u2502  Provider  \u2502\u2500\u2500\u2500\u25ba\u2502   HTTP     \u2502\u2500\u2500\u2500\u25ba\u2502  LLM API   \u2502\n\u2502   Code     \u2502    \u2502  Adapter   \u2502    \u2502   Client   \u2502    \u2502  Backend   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      \u2502                 \u2502                 \u2502                 \u2502\n      \u2502  Unified API    \u2502  Convert to     \u2502  Native HTTP    \u2502\n      \u2502  Request        \u2502  Provider       \u2502  Request        \u2502\n      \u2502                 \u2502  Format         \u2502                 \u2502\n      \u25bc                 \u25bc                 \u25bc                 \u25bc\n</code></pre>"},{"location":"architecture/#adding-new-providers","title":"Adding New Providers","text":""},{"location":"architecture/#external-recommended","title":"External (Recommended)","text":"<ol> <li>Create a new Go module</li> <li>Import <code>github.com/plexusone/omnillm/provider</code></li> <li>Implement the <code>Provider</code> interface</li> <li>Users inject via <code>CustomProvider</code></li> </ol>"},{"location":"architecture/#built-in-core-contributors","title":"Built-in (Core Contributors)","text":"<ol> <li>Create provider package: <code>providers/newprovider/</code></li> <li>Implement HTTP client, types, and adapter</li> <li>Add factory function in <code>providers.go</code></li> <li>Add provider constant</li> </ol>"},{"location":"testing/","title":"Testing","text":"<p>OmniLLM includes a comprehensive test suite with both unit tests and integration tests.</p>"},{"location":"testing/#running-tests","title":"Running Tests","text":"<pre><code># Run all unit tests (no API keys required)\ngo test ./... -short\n\n# Run with coverage\ngo test ./... -short -cover\n\n# Run integration tests (requires API keys)\nANTHROPIC_API_KEY=your-key go test ./providers/anthropic -v\nOPENAI_API_KEY=your-key go test ./providers/openai -v\nXAI_API_KEY=your-key go test ./providers/xai -v\n\n# Run all tests including integration\nANTHROPIC_API_KEY=your-key OPENAI_API_KEY=your-key XAI_API_KEY=your-key go test ./... -v\n</code></pre>"},{"location":"testing/#test-coverage","title":"Test Coverage","text":"<ul> <li>Unit Tests: Mock-based tests that run without external dependencies</li> <li>Integration Tests: Real API tests that skip gracefully when API keys are not set</li> <li>Memory Tests: Comprehensive conversation memory management tests</li> <li>Provider Tests: Adapter logic, message conversion, and streaming tests</li> </ul>"},{"location":"testing/#writing-tests","title":"Writing Tests","text":"<p>The clean interface design makes testing straightforward:</p> <pre><code>// Mock the Provider interface for testing\ntype mockProvider struct{}\n\nfunc (m *mockProvider) CreateChatCompletion(ctx context.Context, req *omnillm.ChatCompletionRequest) (*omnillm.ChatCompletionResponse, error) {\n    return &amp;omnillm.ChatCompletionResponse{\n        Choices: []omnillm.ChatCompletionChoice{\n            {\n                Message: omnillm.Message{\n                    Role:    omnillm.RoleAssistant,\n                    Content: \"Mock response\",\n                },\n            },\n        },\n    }, nil\n}\n\nfunc (m *mockProvider) CreateChatCompletionStream(ctx context.Context, req *omnillm.ChatCompletionRequest) (omnillm.ChatCompletionStream, error) {\n    return nil, nil\n}\n\nfunc (m *mockProvider) Close() error { return nil }\nfunc (m *mockProvider) Name() string { return \"mock\" }\n</code></pre>"},{"location":"testing/#conditional-integration-tests","title":"Conditional Integration Tests","text":"<p>Integration tests automatically skip when API keys are not available:</p> <pre><code>func TestAnthropicIntegration_Streaming(t *testing.T) {\n    apiKey := os.Getenv(\"ANTHROPIC_API_KEY\")\n    if apiKey == \"\" {\n        t.Skip(\"Skipping integration test: ANTHROPIC_API_KEY not set\")\n    }\n    // Test code here...\n}\n</code></pre>"},{"location":"testing/#mock-kvs-for-memory-testing","title":"Mock KVS for Memory Testing","text":"<p>OmniLLM provides a mock KVS implementation for testing memory functionality:</p> <pre><code>import omnillmtest \"github.com/plexusone/omnillm/testing\"\n\n// Create mock KVS for testing\nmockKVS := omnillmtest.NewMockKVS()\n\nclient, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOpenAI, APIKey: \"test-key\"},\n    },\n    Memory: mockKVS,\n})\n</code></pre>"},{"location":"features/caching/","title":"Response Caching","text":"<p>OmniLLM supports response caching to reduce API costs for identical requests.</p>"},{"location":"features/caching/#basic-usage","title":"Basic Usage","text":"<pre><code>client, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOpenAI, APIKey: \"your-key\"},\n    },\n    Cache: kvsClient, // Your KVS implementation (Redis, DynamoDB, etc.)\n    CacheConfig: &amp;omnillm.CacheConfig{\n        TTL:       1 * time.Hour,\n        KeyPrefix: \"myapp:llm-cache\",\n    },\n})\n\n// First call hits the API\nresponse1, _ := client.CreateChatCompletion(ctx, request)\n\n// Second identical call returns cached response\nresponse2, _ := client.CreateChatCompletion(ctx, request)\n\n// Check if response was from cache\nif response2.ProviderMetadata[\"cache_hit\"] == true {\n    fmt.Println(\"Response was cached!\")\n}\n</code></pre>"},{"location":"features/caching/#configuration","title":"Configuration","text":"<pre><code>cacheConfig := &amp;omnillm.CacheConfig{\n    TTL:                1 * time.Hour,       // Time-to-live\n    KeyPrefix:          \"omnillm:cache\",     // Key prefix\n    SkipStreaming:      true,                // Don't cache streaming (default)\n    CacheableModels:    []string{\"gpt-4o\"},  // Only cache specific models (nil = all)\n    IncludeTemperature: true,                // Temperature affects cache key\n    IncludeSeed:        true,                // Seed affects cache key\n}\n</code></pre>"},{"location":"features/caching/#cache-key-generation","title":"Cache Key Generation","text":"<p>Cache keys are generated from a SHA-256 hash of:</p> <ul> <li>Model name</li> <li>Messages (role, content, name, tool_call_id)</li> <li>MaxTokens, Temperature, TopP, TopK, Seed, Stop sequences</li> </ul> <p>Different parameter values = different cache keys.</p>"},{"location":"features/caching/#cache-backends","title":"Cache Backends","text":"<p>Caching uses the same KVS backend as conversation memory:</p> <ul> <li>Redis: High-performance distributed caching</li> <li>DynamoDB: AWS-native caching</li> <li>In-Memory: Development and testing</li> <li>Custom: Any Sogo KVS implementation</li> </ul>"},{"location":"features/fallback/","title":"Fallback &amp; Circuit Breaker","text":"<p>OmniLLM supports automatic failover to backup providers and circuit breaker patterns to handle provider failures gracefully.</p>"},{"location":"features/fallback/#fallback-providers","title":"Fallback Providers","text":"<pre><code>// Providers[0] is primary, Providers[1+] are fallbacks\nclient, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOpenAI, APIKey: \"openai-key\"},       // Primary\n        {Provider: omnillm.ProviderNameAnthropic, APIKey: \"anthropic-key\"}, // Fallback 1\n        {Provider: omnillm.ProviderNameGemini, APIKey: \"gemini-key\"},       // Fallback 2\n    },\n})\n\n// If OpenAI fails with a retryable error, automatically tries Anthropic, then Gemini\nresponse, err := client.CreateChatCompletion(ctx, request)\n</code></pre>"},{"location":"features/fallback/#error-classification","title":"Error Classification","text":"<p>Fallback uses intelligent error classification:</p> Error Type Triggers Fallback Rate limits (429) Yes Server errors (5xx) Yes Network errors Yes Auth errors (401/403) No Invalid requests (400) No"},{"location":"features/fallback/#circuit-breaker","title":"Circuit Breaker","text":"<p>The circuit breaker pattern prevents cascading failures by temporarily skipping providers that are unhealthy.</p>"},{"location":"features/fallback/#configuration","title":"Configuration","text":"<pre><code>client, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOpenAI, APIKey: \"openai-key\"},\n        {Provider: omnillm.ProviderNameAnthropic, APIKey: \"anthropic-key\"},\n    },\n    CircuitBreakerConfig: &amp;omnillm.CircuitBreakerConfig{\n        FailureThreshold:     5,               // Open after 5 consecutive failures\n        SuccessThreshold:     2,               // Close after 2 successes in half-open\n        Timeout:              30 * time.Second, // Wait before trying again\n        FailureRateThreshold: 0.5,             // 50% failure rate opens circuit\n        MinimumRequests:      10,              // Minimum requests for rate calculation\n    },\n})\n</code></pre>"},{"location":"features/fallback/#states","title":"States","text":"<ul> <li>Closed: Normal operation, requests flow through</li> <li>Open: Provider is failing, requests skip it immediately</li> <li>Half-Open: Testing if provider has recovered</li> </ul>"},{"location":"features/fallback/#state-transitions","title":"State Transitions","text":"<pre><code>         success          failure\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502              \u2502   \u2502              \u2502\n   \u25bc              \u2502   \u25bc              \u2502\nCLOSED \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba OPEN \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba HALF-OPEN\n   \u25b2    failures     \u2502   timeout      \u2502\n   \u2502                 \u2502                \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         success         failure\n</code></pre>"},{"location":"features/memory/","title":"Conversation Memory","text":"<p>OmniLLM supports persistent conversation memory using any Key-Value Store that implements the Sogo KVS interface.</p>"},{"location":"features/memory/#configuration","title":"Configuration","text":"<pre><code>memoryConfig := omnillm.MemoryConfig{\n    MaxMessages: 50,                    // Keep last 50 messages per session\n    TTL:         24 * time.Hour,        // Messages expire after 24 hours\n    KeyPrefix:   \"myapp:conversations\", // Custom key prefix\n}\n\nclient, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOpenAI, APIKey: \"your-api-key\"},\n    },\n    Memory:       kvsClient,   // Your KVS implementation\n    MemoryConfig: &amp;memoryConfig,\n})\n</code></pre>"},{"location":"features/memory/#memory-aware-completions","title":"Memory-Aware Completions","text":"<pre><code>// Create a session with system message\nerr = client.CreateConversationWithSystemMessage(ctx, \"user-123\",\n    \"You are a helpful assistant that remembers our conversation history.\")\n\n// Use memory-aware completion - automatically loads conversation history\nresponse, err := client.CreateChatCompletionWithMemory(ctx, \"user-123\", &amp;omnillm.ChatCompletionRequest{\n    Model: omnillm.ModelGPT4o,\n    Messages: []omnillm.Message{\n        {Role: omnillm.RoleUser, Content: \"What did we discuss last time?\"},\n    },\n    MaxTokens: &amp;[]int{200}[0],\n})\n</code></pre>"},{"location":"features/memory/#memory-management","title":"Memory Management","text":"<pre><code>// Load conversation history\nconversation, err := client.LoadConversation(ctx, \"user-123\")\n\n// Get just the messages\nmessages, err := client.GetConversationMessages(ctx, \"user-123\")\n\n// Manually append messages\nerr = client.AppendMessage(ctx, \"user-123\", omnillm.Message{\n    Role:    omnillm.RoleUser,\n    Content: \"Remember this important fact: I prefer JSON responses.\",\n})\n\n// Delete conversation\nerr = client.DeleteConversation(ctx, \"user-123\")\n</code></pre>"},{"location":"features/memory/#kvs-backend-support","title":"KVS Backend Support","text":"<p>Memory works with any KVS implementation:</p> <ul> <li>Redis: High-performance, distributed memory</li> <li>DynamoDB: AWS-native storage</li> <li>In-Memory: Testing and development</li> <li>Custom: Any implementation of the Sogo KVS interface</li> </ul> <pre><code>// Example with Redis\nredisKVS := redis.NewKVSClient(\"localhost:6379\")\nclient, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOpenAI, APIKey: \"your-key\"},\n    },\n    Memory: redisKVS,\n})\n</code></pre>"},{"location":"features/memory/#mock-kvs-for-testing","title":"Mock KVS for Testing","text":"<pre><code>import omnillmtest \"github.com/plexusone/omnillm/testing\"\n\nmockKVS := omnillmtest.NewMockKVS()\n\nclient, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOpenAI, APIKey: \"test-key\"},\n    },\n    Memory: mockKVS,\n})\n</code></pre>"},{"location":"features/observability/","title":"Observability","text":"<p>OmniLLM supports observability hooks for tracing, logging, and metrics without modifying the core library.</p>"},{"location":"features/observability/#observabilityhook-interface","title":"ObservabilityHook Interface","text":"<pre><code>type ObservabilityHook interface {\n    // BeforeRequest is called before each LLM call\n    BeforeRequest(ctx context.Context, info LLMCallInfo, req *provider.ChatCompletionRequest) context.Context\n\n    // AfterResponse is called after each LLM call completes\n    AfterResponse(ctx context.Context, info LLMCallInfo, req *provider.ChatCompletionRequest, resp *provider.ChatCompletionResponse, err error)\n\n    // WrapStream wraps a stream for observability\n    WrapStream(ctx context.Context, info LLMCallInfo, req *provider.ChatCompletionRequest, stream provider.ChatCompletionStream) provider.ChatCompletionStream\n}\n\ntype LLMCallInfo struct {\n    CallID       string    // Unique identifier for correlating\n    ProviderName string    // e.g., \"openai\", \"anthropic\"\n    StartTime    time.Time // When the call started\n}\n</code></pre>"},{"location":"features/observability/#simple-logging-hook","title":"Simple Logging Hook","text":"<pre><code>type LoggingHook struct{}\n\nfunc (h *LoggingHook) BeforeRequest(ctx context.Context, info omnillm.LLMCallInfo, req *omnillm.ChatCompletionRequest) context.Context {\n    log.Printf(\"[%s] LLM call started: provider=%s model=%s\", info.CallID, info.ProviderName, req.Model)\n    return ctx\n}\n\nfunc (h *LoggingHook) AfterResponse(ctx context.Context, info omnillm.LLMCallInfo, req *omnillm.ChatCompletionRequest, resp *omnillm.ChatCompletionResponse, err error) {\n    duration := time.Since(info.StartTime)\n    if err != nil {\n        log.Printf(\"[%s] LLM call failed: duration=%v error=%v\", info.CallID, duration, err)\n    } else {\n        log.Printf(\"[%s] LLM call completed: duration=%v tokens=%d\", info.CallID, duration, resp.Usage.TotalTokens)\n    }\n}\n\nfunc (h *LoggingHook) WrapStream(ctx context.Context, info omnillm.LLMCallInfo, req *omnillm.ChatCompletionRequest, stream omnillm.ChatCompletionStream) omnillm.ChatCompletionStream {\n    return stream\n}\n\n// Use the hook\nclient, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOpenAI, APIKey: \"your-api-key\"},\n    },\n    ObservabilityHook: &amp;LoggingHook{},\n})\n</code></pre>"},{"location":"features/observability/#opentelemetry-integration","title":"OpenTelemetry Integration","text":"<pre><code>type OTelHook struct {\n    tracer trace.Tracer\n}\n\nfunc (h *OTelHook) BeforeRequest(ctx context.Context, info omnillm.LLMCallInfo, req *omnillm.ChatCompletionRequest) context.Context {\n    ctx, span := h.tracer.Start(ctx, \"llm.chat_completion\",\n        trace.WithAttributes(\n            attribute.String(\"llm.provider\", info.ProviderName),\n            attribute.String(\"llm.model\", req.Model),\n        ),\n    )\n    return ctx\n}\n\nfunc (h *OTelHook) AfterResponse(ctx context.Context, info omnillm.LLMCallInfo, req *omnillm.ChatCompletionRequest, resp *omnillm.ChatCompletionResponse, err error) {\n    span := trace.SpanFromContext(ctx)\n    defer span.End()\n\n    if err != nil {\n        span.RecordError(err)\n        span.SetStatus(codes.Error, err.Error())\n    } else if resp != nil {\n        span.SetAttributes(\n            attribute.Int(\"llm.tokens.total\", resp.Usage.TotalTokens),\n            attribute.Int(\"llm.tokens.prompt\", resp.Usage.PromptTokens),\n            attribute.Int(\"llm.tokens.completion\", resp.Usage.CompletionTokens),\n        )\n    }\n}\n</code></pre>"},{"location":"features/observability/#omniobserve-integration","title":"OmniObserve Integration","text":"<p>For full LLM observability, use OmniObserve:</p> <pre><code>import \"github.com/plexusone/omniobserve/integrations/omnillm\"\n\n// Create omnillm hook from omniobserve provider\nhook := omnillm.NewHook(omniobserveProvider)\n\nclient, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers:         []omnillm.ProviderConfig{...},\n    ObservabilityHook: hook,\n})\n</code></pre>"},{"location":"features/retry/","title":"Retry with Backoff","text":"<p>OmniLLM supports automatic retries for transient failures via a custom HTTP client.</p>"},{"location":"features/retry/#configuration","title":"Configuration","text":"<pre><code>import (\n    \"net/http\"\n    \"time\"\n\n    \"github.com/plexusone/omnillm\"\n    \"github.com/grokify/mogo/net/http/retryhttp\"\n)\n\n// Create retry transport with exponential backoff\nrt := retryhttp.NewWithOptions(\n    retryhttp.WithMaxRetries(5),\n    retryhttp.WithInitialBackoff(500 * time.Millisecond),\n    retryhttp.WithMaxBackoff(30 * time.Second),\n    retryhttp.WithOnRetry(func(attempt int, req *http.Request, resp *http.Response, err error, backoff time.Duration) {\n        log.Printf(\"Retry attempt %d, waiting %v\", attempt, backoff)\n    }),\n)\n\n// Create client with retry-enabled HTTP client\nclient, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {\n            Provider: omnillm.ProviderNameOpenAI,\n            APIKey:   os.Getenv(\"OPENAI_API_KEY\"),\n            HTTPClient: &amp;http.Client{\n                Transport: rt,\n                Timeout:   2 * time.Minute,\n            },\n        },\n    },\n})\n</code></pre>"},{"location":"features/retry/#retry-transport-options","title":"Retry Transport Options","text":"Option Default Description Max Retries 3 Maximum retry attempts Initial Backoff 1s Starting backoff duration Max Backoff 30s Cap on backoff duration Backoff Multiplier 2.0 Exponential growth factor Jitter 10% Randomness to prevent thundering herd Retryable Status Codes 429, 500, 502, 503, 504 Rate limits + 5xx errors"},{"location":"features/retry/#additional-options","title":"Additional Options","text":"<pre><code>retryhttp.WithRetryableStatusCodes([]int{429, 500, 502, 503, 504})\nretryhttp.WithShouldRetry(func(resp *http.Response, err error) bool {\n    // Custom retry logic\n})\nretryhttp.WithLogger(logger)\n</code></pre>"},{"location":"features/retry/#retry-after-header","title":"Retry-After Header","text":"<p>The retry transport automatically respects <code>Retry-After</code> headers from API responses.</p>"},{"location":"features/retry/#provider-support","title":"Provider Support","text":"Provider Custom HTTP Client OpenAI Yes Anthropic Yes X.AI Yes Ollama Yes Gemini No (SDK-managed) Bedrock No (SDK-managed)"},{"location":"features/streaming/","title":"Streaming","text":"<p>OmniLLM supports real-time response streaming for all providers.</p>"},{"location":"features/streaming/#basic-streaming","title":"Basic Streaming","text":"<pre><code>stream, err := client.CreateChatCompletionStream(context.Background(), &amp;omnillm.ChatCompletionRequest{\n    Model: omnillm.ModelGPT4o,\n    Messages: []omnillm.Message{\n        {Role: omnillm.RoleUser, Content: \"Tell me a short story about AI.\"},\n    },\n    MaxTokens:   &amp;[]int{200}[0],\n    Temperature: &amp;[]float64{0.8}[0],\n})\nif err != nil {\n    log.Fatal(err)\n}\ndefer stream.Close()\n\nfmt.Print(\"AI Response: \")\nfor {\n    chunk, err := stream.Recv()\n    if err == io.EOF {\n        break\n    }\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    if len(chunk.Choices) &gt; 0 &amp;&amp; chunk.Choices[0].Delta != nil {\n        fmt.Print(chunk.Choices[0].Delta.Content)\n    }\n}\nfmt.Println()\n</code></pre>"},{"location":"features/streaming/#stream-interface","title":"Stream Interface","text":"<pre><code>type ChatCompletionStream interface {\n    // Recv receives the next chunk from the stream\n    Recv() (*ChatCompletionStreamResponse, error)\n\n    // Close closes the stream\n    Close() error\n}\n</code></pre>"},{"location":"features/streaming/#provider-support","title":"Provider Support","text":"Provider Streaming OpenAI Yes Anthropic Yes (SSE) Google Gemini Yes X.AI Yes Ollama Yes AWS Bedrock Yes"},{"location":"features/streaming/#streaming-with-observability","title":"Streaming with Observability","text":"<p>When using observability hooks, wrap the stream to track streaming metrics:</p> <pre><code>func (h *MyHook) WrapStream(ctx context.Context, info omnillm.LLMCallInfo, req *omnillm.ChatCompletionRequest, stream omnillm.ChatCompletionStream) omnillm.ChatCompletionStream {\n    return &amp;observableStream{\n        stream:    stream,\n        ctx:       ctx,\n        info:      info,\n        startTime: time.Now(),\n    }\n}\n</code></pre>"},{"location":"features/tokens/","title":"Token Estimation","text":"<p>OmniLLM provides pre-flight token estimation to validate requests before sending them to the API.</p>"},{"location":"features/tokens/#basic-usage","title":"Basic Usage","text":"<pre><code>// Create estimator with default config\nestimator := omnillm.NewTokenEstimator(omnillm.DefaultTokenEstimatorConfig())\n\n// Estimate tokens for messages\ntokens, err := estimator.EstimateTokens(\"gpt-4o\", messages)\n\n// Get model's context window\nwindow := estimator.GetContextWindow(\"gpt-4o\") // Returns 128000\n</code></pre>"},{"location":"features/tokens/#automatic-validation","title":"Automatic Validation","text":"<p>Enable automatic token validation in client:</p> <pre><code>client, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOpenAI, APIKey: \"your-key\"},\n    },\n    TokenEstimator: omnillm.NewTokenEstimator(omnillm.DefaultTokenEstimatorConfig()),\n    ValidateTokens: true, // Rejects requests that exceed context window\n})\n\n// Returns TokenLimitError if request exceeds model limits\nresponse, err := client.CreateChatCompletion(ctx, request)\nif tlErr, ok := err.(*omnillm.TokenLimitError); ok {\n    fmt.Printf(\"Request has %d tokens, but model only supports %d\\n\",\n        tlErr.EstimatedTokens, tlErr.ContextWindow)\n}\n</code></pre>"},{"location":"features/tokens/#built-in-context-windows","title":"Built-in Context Windows","text":"Provider Models Context Window OpenAI GPT-4o, GPT-4o-mini 128,000 OpenAI o1 200,000 Anthropic Claude 3/3.5/4 200,000 Google Gemini 2.5 1,000,000 Google Gemini 1.5 Pro 2,000,000 X.AI Grok 3/4 128,000"},{"location":"features/tokens/#custom-configuration","title":"Custom Configuration","text":"<pre><code>config := omnillm.TokenEstimatorConfig{\n    CharactersPerToken: 3.5, // More conservative estimate\n    CustomContextWindows: map[string]int{\n        \"my-custom-model\": 500000,\n        \"gpt-4o\":          200000, // Override built-in\n    },\n}\nestimator := omnillm.NewTokenEstimator(config)\n</code></pre>"},{"location":"features/tools/","title":"Tool Calling","text":"<p>OmniLLM supports function/tool calling for building agentic workflows. Tools allow the LLM to request specific actions that your application can execute.</p>"},{"location":"features/tools/#basic-tool-calling","title":"Basic Tool Calling","text":"<pre><code>response, err := client.CreateChatCompletion(ctx, &amp;omnillm.ChatCompletionRequest{\n    Model: omnillm.ModelGPT4o,\n    Messages: []omnillm.Message{\n        {Role: omnillm.RoleUser, Content: \"What's the weather in Tokyo?\"},\n    },\n    Tools: []omnillm.Tool{\n        {\n            Type: \"function\",\n            Function: omnillm.ToolFunction{\n                Name:        \"get_weather\",\n                Description: \"Get current weather for a location\",\n                Parameters: map[string]any{\n                    \"type\": \"object\",\n                    \"properties\": map[string]any{\n                        \"location\": map[string]any{\n                            \"type\":        \"string\",\n                            \"description\": \"City name\",\n                        },\n                    },\n                    \"required\": []string{\"location\"},\n                },\n            },\n        },\n    },\n})\n</code></pre>"},{"location":"features/tools/#handling-tool-calls","title":"Handling Tool Calls","text":"<p>When the LLM wants to call a tool, the response includes tool calls:</p> <pre><code>if len(response.Choices) &gt; 0 &amp;&amp; len(response.Choices[0].Message.ToolCalls) &gt; 0 {\n    for _, toolCall := range response.Choices[0].Message.ToolCalls {\n        if toolCall.Function.Name == \"get_weather\" {\n            // Parse arguments\n            var args struct {\n                Location string `json:\"location\"`\n            }\n            json.Unmarshal([]byte(toolCall.Function.Arguments), &amp;args)\n\n            // Execute the tool\n            weather := getWeather(args.Location)\n\n            // Send tool result back to LLM\n            response, err = client.CreateChatCompletion(ctx, &amp;omnillm.ChatCompletionRequest{\n                Model: omnillm.ModelGPT4o,\n                Messages: []omnillm.Message{\n                    {Role: omnillm.RoleUser, Content: \"What's the weather in Tokyo?\"},\n                    response.Choices[0].Message, // Assistant message with tool call\n                    {\n                        Role:       omnillm.RoleTool,\n                        Content:    weather,\n                        ToolCallID: &amp;toolCall.ID,\n                    },\n                },\n            })\n        }\n    }\n}\n</code></pre>"},{"location":"features/tools/#provider-support","title":"Provider Support","text":"Provider Tool Calling OpenAI Yes Anthropic Yes X.AI (Grok) Yes Google Gemini Partial Ollama Model-dependent"},{"location":"features/tools/#tool-types","title":"Tool Types","text":"<pre><code>type Tool struct {\n    Type     string       `json:\"type\"`     // \"function\"\n    Function ToolFunction `json:\"function\"`\n}\n\ntype ToolFunction struct {\n    Name        string         `json:\"name\"`\n    Description string         `json:\"description\"`\n    Parameters  map[string]any `json:\"parameters\"` // JSON Schema\n}\n\ntype ToolCall struct {\n    ID       string           `json:\"id\"`\n    Type     string           `json:\"type\"`\n    Function ToolCallFunction `json:\"function\"`\n}\n\ntype ToolCallFunction struct {\n    Name      string `json:\"name\"`\n    Arguments string `json:\"arguments\"` // JSON string\n}\n</code></pre>"},{"location":"getting-started/configuration/","title":"Configuration","text":""},{"location":"getting-started/configuration/#client-configuration","title":"Client Configuration","text":"<pre><code>config := omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {\n            Provider: omnillm.ProviderNameOpenAI,\n            APIKey:   \"your-api-key\",\n            BaseURL:  \"https://custom-endpoint.com/v1\", // Optional\n            Extra: map[string]any{\n                \"timeout\": 60, // Custom provider-specific settings\n            },\n        },\n    },\n}\n</code></pre>"},{"location":"getting-started/configuration/#request-parameters","title":"Request Parameters","text":"<p><code>ChatCompletionRequest</code> supports the following parameters:</p> Parameter Type Providers Description <code>Model</code> <code>string</code> All Model identifier (required) <code>Messages</code> <code>[]Message</code> All Conversation messages (required) <code>MaxTokens</code> <code>*int</code> All Maximum tokens to generate <code>Temperature</code> <code>*float64</code> All Randomness (0.0-2.0) <code>TopP</code> <code>*float64</code> All Nucleus sampling threshold <code>TopK</code> <code>*int</code> Anthropic, Gemini, Ollama Top K token selection <code>Stop</code> <code>[]string</code> All Stop sequences <code>PresencePenalty</code> <code>*float64</code> OpenAI, X.AI Penalize tokens by presence <code>FrequencyPenalty</code> <code>*float64</code> OpenAI, X.AI Penalize tokens by frequency <code>Seed</code> <code>*int</code> OpenAI, X.AI, Ollama Reproducible outputs <code>N</code> <code>*int</code> OpenAI Number of completions <code>ResponseFormat</code> <code>*ResponseFormat</code> OpenAI, Gemini JSON mode <code>Logprobs</code> <code>*bool</code> OpenAI Return log probabilities <code>TopLogprobs</code> <code>*int</code> OpenAI Top logprobs count (0-20) <code>User</code> <code>*string</code> OpenAI End-user identifier <code>LogitBias</code> <code>map[string]int</code> OpenAI Token bias adjustments"},{"location":"getting-started/configuration/#example-advanced-request","title":"Example: Advanced Request","text":"<pre><code>// Helper for pointer values\nfunc ptr[T any](v T) *T { return &amp;v }\n\n// Reproducible outputs with seed\nresponse, err := client.CreateChatCompletion(ctx, &amp;omnillm.ChatCompletionRequest{\n    Model:    omnillm.ModelGPT4o,\n    Messages: messages,\n    Seed:     ptr(42), // Same seed = same output\n})\n\n// JSON mode response\nresponse, err := client.CreateChatCompletion(ctx, &amp;omnillm.ChatCompletionRequest{\n    Model:    omnillm.ModelGPT4o,\n    Messages: messages,\n    ResponseFormat: &amp;omnillm.ResponseFormat{Type: \"json_object\"},\n})\n\n// TopK sampling (Anthropic/Gemini/Ollama)\nresponse, err := client.CreateChatCompletion(ctx, &amp;omnillm.ChatCompletionRequest{\n    Model:    omnillm.ModelClaude3Sonnet,\n    Messages: messages,\n    TopK:     ptr(40), // Consider only top 40 tokens\n})\n</code></pre>"},{"location":"getting-started/configuration/#logging-configuration","title":"Logging Configuration","text":"<p>OmniLLM supports injectable logging via Go's standard <code>log/slog</code> package:</p> <pre><code>import (\n    \"log/slog\"\n    \"os\"\n\n    \"github.com/plexusone/omnillm\"\n)\n\nlogger := slog.New(slog.NewJSONHandler(os.Stderr, &amp;slog.HandlerOptions{\n    Level: slog.LevelDebug,\n}))\n\nclient, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOpenAI, APIKey: \"your-api-key\"},\n    },\n    Logger: logger,\n})\n</code></pre>"},{"location":"getting-started/configuration/#context-aware-logging","title":"Context-Aware Logging","text":"<p>Attach request-scoped attributes to all log output:</p> <pre><code>import \"github.com/grokify/mogo/log/slogutil\"\n\nreqLogger := slog.Default().With(\n    slog.String(\"trace_id\", traceID),\n    slog.String(\"user_id\", userID),\n)\n\nctx = slogutil.ContextWithLogger(ctx, reqLogger)\n\n// All internal logging will now include trace_id and user_id\nresponse, err := client.CreateChatCompletionWithMemory(ctx, sessionID, req)\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Go 1.21 or later</li> </ul>"},{"location":"getting-started/installation/#install","title":"Install","text":"<pre><code>go get github.com/plexusone/omnillm\n</code></pre>"},{"location":"getting-started/installation/#external-providers","title":"External Providers","text":"<p>Some providers with heavy SDK dependencies are available as separate modules:</p> <pre><code># AWS Bedrock (requires AWS SDK)\ngo get github.com/plexusone/omnillm-bedrock\n</code></pre>"},{"location":"getting-started/installation/#environment-variables","title":"Environment Variables","text":"<p>Set API keys for the providers you want to use:</p> <pre><code>export OPENAI_API_KEY=\"your-openai-api-key\"\nexport ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\nexport GEMINI_API_KEY=\"your-gemini-api-key\"\nexport XAI_API_KEY=\"your-xai-api-key\"\n</code></pre> <p>Ollama runs locally and doesn't require an API key.</p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":""},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":"<pre><code>package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n\n    \"github.com/plexusone/omnillm\"\n)\n\nfunc main() {\n    // Create a client for OpenAI\n    client, err := omnillm.NewClient(omnillm.ClientConfig{\n        Providers: []omnillm.ProviderConfig{\n            {Provider: omnillm.ProviderNameOpenAI, APIKey: \"your-openai-api-key\"},\n        },\n    })\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer client.Close()\n\n    // Create a chat completion request\n    response, err := client.CreateChatCompletion(context.Background(), &amp;omnillm.ChatCompletionRequest{\n        Model: omnillm.ModelGPT4o,\n        Messages: []omnillm.Message{\n            {\n                Role:    omnillm.RoleUser,\n                Content: \"Hello! How can you help me today?\",\n            },\n        },\n        MaxTokens:   &amp;[]int{150}[0],\n        Temperature: &amp;[]float64{0.7}[0],\n    })\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    fmt.Printf(\"Response: %s\\n\", response.Choices[0].Message.Content)\n    fmt.Printf(\"Tokens used: %d\\n\", response.Usage.TotalTokens)\n}\n</code></pre>"},{"location":"getting-started/quickstart/#provider-switching","title":"Provider Switching","text":"<p>The unified interface makes it easy to switch between providers:</p> <pre><code>// Same request works with any provider\nrequest := &amp;omnillm.ChatCompletionRequest{\n    Model: omnillm.ModelGPT4o, // or omnillm.ModelClaude3Sonnet, etc.\n    Messages: []omnillm.Message{\n        {Role: omnillm.RoleUser, Content: \"Hello, world!\"},\n    },\n    MaxTokens: &amp;[]int{100}[0],\n}\n\n// OpenAI\nopenaiClient, _ := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOpenAI, APIKey: \"openai-key\"},\n    },\n})\n\n// Anthropic\nanthropicClient, _ := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameAnthropic, APIKey: \"anthropic-key\"},\n    },\n})\n\n// Same API call for all providers\nresponse1, _ := openaiClient.CreateChatCompletion(ctx, request)\nresponse2, _ := anthropicClient.CreateChatCompletion(ctx, request)\n</code></pre>"},{"location":"getting-started/quickstart/#running-examples","title":"Running Examples","text":"<p>The repository includes comprehensive examples:</p> <pre><code>go run examples/basic/main.go\ngo run examples/streaming/main.go\ngo run examples/conversation/main.go\ngo run examples/memory_demo/main.go\n</code></pre>"},{"location":"providers/","title":"Providers","text":"<p>OmniLLM supports multiple LLM providers through a unified interface. Each provider is configured via <code>ProviderConfig</code> and implements the same <code>Provider</code> interface.</p>"},{"location":"providers/#built-in-providers","title":"Built-in Providers","text":"Provider Package Description OpenAI Built-in GPT-5, GPT-4o, GPT-4-turbo, GPT-3.5-turbo Anthropic Built-in Claude Opus 4, Sonnet 4, Claude 3.x series Google Gemini Built-in Gemini 2.5/1.5 Pro and Flash X.AI Built-in Grok 4, Grok 3, 2M context window Ollama Built-in Local models (Llama, Mistral, etc.)"},{"location":"providers/#external-providers","title":"External Providers","text":"<p>Some providers with heavy SDK dependencies are available as separate modules:</p> Provider Module Why External AWS Bedrock <code>github.com/plexusone/omnillm-bedrock</code> AWS SDK v2 adds 17+ transitive dependencies"},{"location":"providers/#multi-provider-configuration","title":"Multi-Provider Configuration","text":"<p>Configure multiple providers for fallback support:</p> <pre><code>client, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOpenAI, APIKey: \"openai-key\"},       // Primary\n        {Provider: omnillm.ProviderNameAnthropic, APIKey: \"anthropic-key\"}, // Fallback 1\n        {Provider: omnillm.ProviderNameGemini, APIKey: \"gemini-key\"},       // Fallback 2\n    },\n})\n\n// If OpenAI fails with a retryable error, automatically tries Anthropic, then Gemini\nresponse, err := client.CreateChatCompletion(ctx, request)\n</code></pre>"},{"location":"providers/#model-support-summary","title":"Model Support Summary","text":"Provider Models Context Window Features OpenAI GPT-5, GPT-4.1, GPT-4o 128K-200K Chat, Streaming, Tools Anthropic Claude Opus 4, Sonnet 4 200K Chat, Streaming, System Gemini Gemini 2.5 Pro/Flash 1M-2M Chat, Streaming X.AI Grok 4, Grok 3 128K-2M Chat, Streaming, Tools Ollama Llama 3, Mistral Varies Chat, Streaming, Local Bedrock Claude, Titan Varies Chat"},{"location":"providers/anthropic/","title":"Anthropic (Claude)","text":""},{"location":"providers/anthropic/#overview","title":"Overview","text":"<ul> <li>Models: Claude-Opus-4.1, Claude-Opus-4, Claude-Sonnet-4, Claude-3.7-Sonnet, Claude-3.5-Haiku, Claude-3-Opus, Claude-3-Sonnet, Claude-3-Haiku</li> <li>Features: Chat completions, streaming, system message support</li> </ul>"},{"location":"providers/anthropic/#configuration","title":"Configuration","text":"<pre><code>client, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameAnthropic, APIKey: \"your-anthropic-api-key\"},\n    },\n})\n</code></pre>"},{"location":"providers/anthropic/#available-models","title":"Available Models","text":"Model Context Window Description <code>omnillm.ModelClaudeOpus41</code> 200K Claude Opus 4.1 (most capable) <code>omnillm.ModelClaudeOpus4</code> 200K Claude Opus 4 <code>omnillm.ModelClaudeSonnet4</code> 200K Claude Sonnet 4 <code>omnillm.ModelClaude37Sonnet</code> 200K Claude 3.7 Sonnet <code>omnillm.ModelClaude35Haiku</code> 200K Claude 3.5 Haiku (fast) <code>omnillm.ModelClaude3Opus</code> 200K Claude 3 Opus <code>omnillm.ModelClaude3Sonnet</code> 200K Claude 3 Sonnet <code>omnillm.ModelClaude3Haiku</code> 200K Claude 3 Haiku"},{"location":"providers/anthropic/#topk-sampling","title":"TopK Sampling","text":"<p>Anthropic supports TopK sampling:</p> <pre><code>response, err := client.CreateChatCompletion(ctx, &amp;omnillm.ChatCompletionRequest{\n    Model: omnillm.ModelClaude3Sonnet,\n    Messages: messages,\n    TopK: &amp;[]int{40}[0], // Consider only top 40 tokens\n})\n</code></pre>"},{"location":"providers/anthropic/#system-messages","title":"System Messages","text":"<p>System messages are fully supported:</p> <pre><code>messages := []omnillm.Message{\n    {Role: omnillm.RoleSystem, Content: \"You are a helpful assistant.\"},\n    {Role: omnillm.RoleUser, Content: \"Hello!\"},\n}\n</code></pre>"},{"location":"providers/bedrock/","title":"AWS Bedrock","text":""},{"location":"providers/bedrock/#overview","title":"Overview","text":"<p>AWS Bedrock is available as an external module to avoid pulling AWS SDK dependencies for users who don't need it.</p> <ul> <li>Models: Claude models, Titan models</li> <li>Features: Chat completions, multiple model families</li> </ul>"},{"location":"providers/bedrock/#installation","title":"Installation","text":"<pre><code>go get github.com/plexusone/omnillm-bedrock\n</code></pre>"},{"location":"providers/bedrock/#configuration","title":"Configuration","text":"<pre><code>import (\n    \"github.com/plexusone/omnillm\"\n    \"github.com/plexusone/omnillm-bedrock\"\n)\n\n// Create the Bedrock provider\nbedrockProvider, err := bedrock.NewProvider(\"us-east-1\")\nif err != nil {\n    log.Fatal(err)\n}\n\n// Use it with omnillm via CustomProvider\nclient, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {CustomProvider: bedrockProvider},\n    },\n})\n</code></pre>"},{"location":"providers/bedrock/#aws-credentials","title":"AWS Credentials","text":"<p>The Bedrock provider uses the standard AWS credential chain:</p> <ol> <li>Environment variables (<code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>)</li> <li>Shared credentials file (<code>~/.aws/credentials</code>)</li> <li>IAM role (when running on AWS)</li> </ol>"},{"location":"providers/bedrock/#why-external","title":"Why External?","text":"<p>AWS SDK v2 adds 17+ transitive dependencies. By keeping Bedrock as an external module, users who don't need AWS can keep their dependency tree lean.</p>"},{"location":"providers/bedrock/#source-code","title":"Source Code","text":"<p>See github.com/plexusone/omnillm-bedrock for the full implementation.</p>"},{"location":"providers/custom/","title":"Custom Providers","text":"<p>External packages can create providers without modifying the core library. This is the recommended approach for adding new LLM backends.</p>"},{"location":"providers/custom/#creating-a-provider","title":"Creating a Provider","text":""},{"location":"providers/custom/#step-1-implement-the-provider-interface","title":"Step 1: Implement the Provider Interface","text":"<pre><code>// In your external package (e.g., github.com/yourname/omnillm-myprovider)\npackage myprovider\n\nimport (\n    \"context\"\n    \"github.com/plexusone/omnillm/provider\"\n)\n\n// HTTP Client\ntype Client struct {\n    apiKey string\n    // your HTTP client implementation\n}\n\nfunc New(apiKey string) *Client {\n    return &amp;Client{apiKey: apiKey}\n}\n\n// Provider Adapter\ntype Provider struct {\n    client *Client\n}\n\nfunc NewProvider(apiKey string) provider.Provider {\n    return &amp;Provider{client: New(apiKey)}\n}\n\nfunc (p *Provider) Name() string { return \"myprovider\" }\nfunc (p *Provider) Close() error { return p.client.Close() }\n\nfunc (p *Provider) CreateChatCompletion(ctx context.Context, req *provider.ChatCompletionRequest) (*provider.ChatCompletionResponse, error) {\n    // Convert provider.ChatCompletionRequest to your API format\n    // Make HTTP call via p.client\n    // Convert response back to provider.ChatCompletionResponse\n}\n\nfunc (p *Provider) CreateChatCompletionStream(ctx context.Context, req *provider.ChatCompletionRequest) (provider.ChatCompletionStream, error) {\n    // Your streaming implementation\n}\n</code></pre>"},{"location":"providers/custom/#step-2-use-your-provider","title":"Step 2: Use Your Provider","text":"<pre><code>import (\n    \"github.com/plexusone/omnillm\"\n    \"github.com/yourname/omnillm-myprovider\"\n)\n\nfunc main() {\n    customProvider := myprovider.NewProvider(\"your-api-key\")\n\n    client, err := omnillm.NewClient(omnillm.ClientConfig{\n        Providers: []omnillm.ProviderConfig{\n            {CustomProvider: customProvider},\n        },\n    })\n\n    // Use the same omnillm API\n    response, err := client.CreateChatCompletion(ctx, &amp;omnillm.ChatCompletionRequest{\n        Model: \"my-model\",\n        Messages: []omnillm.Message{{Role: omnillm.RoleUser, Content: \"Hello!\"}},\n    })\n}\n</code></pre>"},{"location":"providers/custom/#provider-interface","title":"Provider Interface","text":"<pre><code>type Provider interface {\n    // Name returns the provider name (e.g., \"openai\", \"anthropic\")\n    Name() string\n\n    // CreateChatCompletion performs a synchronous chat completion\n    CreateChatCompletion(ctx context.Context, req *ChatCompletionRequest) (*ChatCompletionResponse, error)\n\n    // CreateChatCompletionStream performs a streaming chat completion\n    CreateChatCompletionStream(ctx context.Context, req *ChatCompletionRequest) (ChatCompletionStream, error)\n\n    // Close releases any resources\n    Close() error\n}\n</code></pre>"},{"location":"providers/custom/#reference-implementations","title":"Reference Implementations","text":"<p>Look at any built-in provider as a reference:</p> <ul> <li><code>providers/openai/</code> - OpenAI implementation</li> <li><code>providers/anthropic/</code> - Anthropic implementation</li> <li>omnillm-bedrock - External provider example</li> </ul>"},{"location":"providers/custom/#benefits","title":"Benefits","text":"<ul> <li>No Core Changes: External providers don't require modifying the core library</li> <li>Clean Injection: Use <code>ProviderConfig.CustomProvider</code> to inject your provider</li> <li>Same Interface: Both internal and external providers use the same <code>provider.Provider</code> interface</li> <li>Easy Testing: Mock the provider interface for unit tests</li> </ul>"},{"location":"providers/gemini/","title":"Google Gemini","text":""},{"location":"providers/gemini/#overview","title":"Overview","text":"<ul> <li>Models: Gemini-2.5-Pro, Gemini-2.5-Flash, Gemini-1.5-Pro, Gemini-1.5-Flash</li> <li>Features: Chat completions, streaming, massive context windows</li> </ul>"},{"location":"providers/gemini/#configuration","title":"Configuration","text":"<pre><code>client, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameGemini, APIKey: \"your-gemini-api-key\"},\n    },\n})\n</code></pre>"},{"location":"providers/gemini/#available-models","title":"Available Models","text":"Model Context Window Description <code>omnillm.ModelGemini25Pro</code> 1M Gemini 2.5 Pro <code>omnillm.ModelGemini25Flash</code> 1M Gemini 2.5 Flash (fast) <code>omnillm.ModelGemini15Pro</code> 2M Gemini 1.5 Pro (largest context) <code>omnillm.ModelGemini15Flash</code> 1M Gemini 1.5 Flash"},{"location":"providers/gemini/#json-mode","title":"JSON Mode","text":"<p>Gemini supports JSON mode for structured outputs:</p> <pre><code>response, err := client.CreateChatCompletion(ctx, &amp;omnillm.ChatCompletionRequest{\n    Model: omnillm.ModelGemini25Pro,\n    Messages: messages,\n    ResponseFormat: &amp;omnillm.ResponseFormat{Type: \"json_object\"},\n})\n</code></pre>"},{"location":"providers/gemini/#large-context","title":"Large Context","text":"<p>Gemini 1.5 Pro supports up to 2 million tokens of context, making it ideal for:</p> <ul> <li>Long document analysis</li> <li>Large codebase understanding</li> <li>Extended conversation history</li> </ul>"},{"location":"providers/ollama/","title":"Ollama","text":""},{"location":"providers/ollama/#overview","title":"Overview","text":"<ul> <li>Models: Llama 3, Mistral, CodeLlama, Gemma, Qwen2.5, DeepSeek-Coder</li> <li>Features: Local inference, no API keys required, optimized for Apple Silicon</li> </ul>"},{"location":"providers/ollama/#configuration","title":"Configuration","text":"<pre><code>client, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOllama, BaseURL: \"http://localhost:11434\"},\n    },\n})\n</code></pre>"},{"location":"providers/ollama/#running-ollama","title":"Running Ollama","text":"<ol> <li>Install Ollama from ollama.ai</li> <li>Pull a model: <code>ollama pull llama3</code></li> <li>Ollama runs automatically on <code>localhost:11434</code></li> </ol>"},{"location":"providers/ollama/#available-models","title":"Available Models","text":"Model Description <code>llama3</code> Meta's Llama 3 <code>llama3:70b</code> Llama 3 70B (larger) <code>mistral</code> Mistral 7B <code>codellama</code> Code-specialized Llama <code>gemma</code> Google's Gemma <code>qwen2.5</code> Alibaba's Qwen 2.5 <code>deepseek-coder</code> DeepSeek Coder"},{"location":"providers/ollama/#example","title":"Example","text":"<pre><code>response, err := client.CreateChatCompletion(ctx, &amp;omnillm.ChatCompletionRequest{\n    Model: \"llama3\",\n    Messages: []omnillm.Message{\n        {Role: omnillm.RoleUser, Content: \"Explain quantum computing simply.\"},\n    },\n})\n</code></pre>"},{"location":"providers/ollama/#streaming","title":"Streaming","text":"<pre><code>stream, err := client.CreateChatCompletionStream(ctx, &amp;omnillm.ChatCompletionRequest{\n    Model: \"llama3\",\n    Messages: messages,\n})\n\nfor {\n    chunk, err := stream.Recv()\n    if err == io.EOF {\n        break\n    }\n    fmt.Print(chunk.Choices[0].Delta.Content)\n}\n</code></pre>"},{"location":"providers/ollama/#custom-ollama-server","title":"Custom Ollama Server","text":"<p>Connect to a remote Ollama instance:</p> <pre><code>{Provider: omnillm.ProviderNameOllama, BaseURL: \"http://192.168.1.100:11434\"}\n</code></pre>"},{"location":"providers/openai/","title":"OpenAI","text":""},{"location":"providers/openai/#overview","title":"Overview","text":"<ul> <li>Models: GPT-5, GPT-4.1, GPT-4o, GPT-4o-mini, GPT-4-turbo, GPT-3.5-turbo</li> <li>Features: Chat completions, streaming, function/tool calling</li> </ul>"},{"location":"providers/openai/#configuration","title":"Configuration","text":"<pre><code>client, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOpenAI, APIKey: \"your-openai-api-key\"},\n    },\n})\n</code></pre>"},{"location":"providers/openai/#available-models","title":"Available Models","text":"Model Context Window Description <code>omnillm.ModelGPT5</code> 200K Latest GPT-5 model <code>omnillm.ModelGPT41</code> 128K GPT-4.1 <code>omnillm.ModelGPT4o</code> 128K GPT-4o (recommended) <code>omnillm.ModelGPT4oMini</code> 128K GPT-4o Mini (cost-effective) <code>omnillm.ModelGPT4Turbo</code> 128K GPT-4 Turbo <code>omnillm.ModelGPT35Turbo</code> 16K GPT-3.5 Turbo"},{"location":"providers/openai/#tool-calling","title":"Tool Calling","text":"<p>OpenAI supports function/tool calling for agentic workflows:</p> <pre><code>response, err := client.CreateChatCompletion(ctx, &amp;omnillm.ChatCompletionRequest{\n    Model: omnillm.ModelGPT4o,\n    Messages: []omnillm.Message{\n        {Role: omnillm.RoleUser, Content: \"What's the weather in Tokyo?\"},\n    },\n    Tools: []omnillm.Tool{\n        {\n            Type: \"function\",\n            Function: omnillm.ToolFunction{\n                Name:        \"get_weather\",\n                Description: \"Get current weather for a location\",\n                Parameters: map[string]any{\n                    \"type\": \"object\",\n                    \"properties\": map[string]any{\n                        \"location\": map[string]any{\n                            \"type\":        \"string\",\n                            \"description\": \"City name\",\n                        },\n                    },\n                    \"required\": []string{\"location\"},\n                },\n            },\n        },\n    },\n})\n</code></pre>"},{"location":"providers/openai/#custom-endpoint","title":"Custom Endpoint","text":"<p>Use a custom OpenAI-compatible endpoint:</p> <pre><code>client, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {\n            Provider: omnillm.ProviderNameOpenAI,\n            APIKey:   \"your-api-key\",\n            BaseURL:  \"https://your-custom-endpoint.com/v1\",\n        },\n    },\n})\n</code></pre>"},{"location":"providers/xai/","title":"X.AI (Grok)","text":""},{"location":"providers/xai/#overview","title":"Overview","text":"<ul> <li>Models: Grok-4.1-Fast, Grok-4, Grok-4-Fast, Grok-Code-Fast, Grok-3, Grok-3-Mini, Grok-2, Grok-2-Vision</li> <li>Features: Chat completions, streaming, OpenAI-compatible API, 2M context window</li> </ul>"},{"location":"providers/xai/#configuration","title":"Configuration","text":"<pre><code>client, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameXAI, APIKey: \"your-xai-api-key\"},\n    },\n})\n</code></pre>"},{"location":"providers/xai/#available-models","title":"Available Models","text":"Model Context Window Description <code>omnillm.ModelGrok41Fast</code> 2M Grok 4.1 Fast (reasoning/non-reasoning) <code>omnillm.ModelGrok4</code> 128K Grok 4 (0709) <code>omnillm.ModelGrok4Fast</code> 2M Grok 4 Fast (reasoning/non-reasoning) <code>omnillm.ModelGrokCodeFast</code> 128K Grok Code Fast <code>omnillm.ModelGrok3</code> 128K Grok 3 <code>omnillm.ModelGrok3Mini</code> 128K Grok 3 Mini <code>omnillm.ModelGrok2</code> 128K Grok 2 <code>omnillm.ModelGrok2Vision</code> 128K Grok 2 Vision"},{"location":"providers/xai/#openai-compatibility","title":"OpenAI Compatibility","text":"<p>X.AI uses an OpenAI-compatible API, so parameters like <code>Seed</code>, <code>PresencePenalty</code>, and <code>FrequencyPenalty</code> are supported:</p> <pre><code>response, err := client.CreateChatCompletion(ctx, &amp;omnillm.ChatCompletionRequest{\n    Model: omnillm.ModelGrok4Fast,\n    Messages: messages,\n    Seed: &amp;[]int{42}[0],\n    PresencePenalty: &amp;[]float64{0.5}[0],\n})\n</code></pre>"},{"location":"releases/changelog/","title":"Changelog","text":"<p>All notable changes to OmniLLM are documented in the individual release notes.</p>"},{"location":"releases/changelog/#releases","title":"Releases","text":"Version Date Highlights v0.12.0 2026-02-22 Tool/function calling support for OpenAI v0.11.0 2026-01-10 Multi-provider configuration, Grok 4 models v0.10.0 2026-01-04 Response caching, token estimation v0.9.0 2026-01-04 Circuit breaker, fallback providers v0.8.0 2026-01-04 Observability hooks v0.7.0 2025-12-26 Conversation memory, X.AI provider v0.6.0 2025-12-26 External provider support, Bedrock"},{"location":"releases/changelog/#versioning","title":"Versioning","text":"<p>OmniLLM follows Semantic Versioning:</p> <ul> <li>MAJOR: Breaking API changes</li> <li>MINOR: New features (backwards compatible)</li> <li>PATCH: Bug fixes (backwards compatible)</li> </ul>"},{"location":"releases/changelog/#full-changelog","title":"Full Changelog","text":"<p>See CHANGELOG.md for the complete changelog in Keep a Changelog format.</p>"},{"location":"releases/v0.10.0/","title":"Release Notes - OmniLLM v0.10.0","text":"<p>Release Date: 2026-01-04 Base Version: v0.9.0</p>"},{"location":"releases/v0.10.0/#overview","title":"Overview","text":"<p>Version 0.10.0 adds configurable HTTP client timeout support and Claude 4.5 model constants. The timeout configuration is essential for reasoning models that may take longer to respond.</p> <p>Summary:</p> <ul> <li>Configurable Timeout: New <code>ClientConfig.Timeout</code> field for setting HTTP client timeout</li> <li>Claude 4.5 Models: Added model constants for Claude Opus 4.5, Sonnet 4.5, and Haiku 4.5</li> <li>HTTP Client Refactor: Unified HTTP client creation with <code>getHTTPClient()</code> helper</li> </ul>"},{"location":"releases/v0.10.0/#new-features","title":"New Features","text":""},{"location":"releases/v0.10.0/#1-configurable-http-client-timeout","title":"1. Configurable HTTP Client Timeout","text":"<p>Added <code>Timeout</code> field to <code>ClientConfig</code> for configuring HTTP client timeout. This is particularly important for reasoning models (like xAI Grok reasoning models) that may need more time to complete.</p> <p>Usage: <pre><code>import (\n    \"time\"\n    \"github.com/plexusone/omnillm\"\n)\n\nclient, err := omnillm.NewClient(omnillm.ClientConfig{\n    Provider: omnillm.ProviderNameXAI,\n    APIKey:   os.Getenv(\"XAI_API_KEY\"),\n    Timeout:  300 * time.Second, // 5 minutes for reasoning models\n})\n</code></pre></p> <p>Behavior:</p> <ul> <li>If <code>Timeout</code> is set and <code>HTTPClient</code> is nil, a new HTTP client is created with the specified timeout</li> <li>If <code>HTTPClient</code> is provided, it takes precedence (for custom transports with retry logic, etc.)</li> <li>If neither is set, providers use their default timeouts</li> </ul> <p>Recommendation: Use <code>300 * time.Second</code> (5 minutes) for reasoning models.</p>"},{"location":"releases/v0.10.0/#2-claude-45-model-constants","title":"2. Claude 4.5 Model Constants","text":"<p>Added model constants for the Claude 4.5 family:</p> <pre><code>import \"github.com/plexusone/omnillm/models\"\n\n// Available Claude 4.5 models\nmodels.ClaudeOpus4_5    // \"claude-opus-4-5-20251101\"\nmodels.ClaudeSonnet4_5  // \"claude-sonnet-4-5-20250929\"\nmodels.ClaudeHaiku4_5   // \"claude-haiku-4-5-20251001\"\n</code></pre>"},{"location":"releases/v0.10.0/#3-documentation","title":"3. Documentation","text":"<p>Added Marp presentation and GitHub Pages HTML documentation for the project.</p>"},{"location":"releases/v0.10.0/#improvements","title":"Improvements","text":""},{"location":"releases/v0.10.0/#http-client-handling-refactor","title":"HTTP Client Handling Refactor","text":"<p>The HTTP client creation logic has been refactored with a new <code>getHTTPClient()</code> helper function that:</p> <ol> <li>Returns the custom <code>HTTPClient</code> if provided</li> <li>Creates a new client with <code>Timeout</code> if specified</li> <li>Returns nil to let providers use their defaults</li> </ol> <p>This provides a unified and consistent HTTP client handling across all providers (OpenAI, Anthropic, xAI, Ollama).</p>"},{"location":"releases/v0.10.0/#upgrade-guide","title":"Upgrade Guide","text":""},{"location":"releases/v0.10.0/#from-v090","title":"From v0.9.0","text":"<p>No breaking changes. To use the new timeout feature:</p> <pre><code>// Before (provider default timeout)\nclient, err := omnillm.NewClient(omnillm.ClientConfig{\n    Provider: omnillm.ProviderNameXAI,\n    APIKey:   apiKey,\n})\n\n// After (custom timeout for reasoning models)\nclient, err := omnillm.NewClient(omnillm.ClientConfig{\n    Provider: omnillm.ProviderNameXAI,\n    APIKey:   apiKey,\n    Timeout:  300 * time.Second,\n})\n</code></pre> <pre><code>go get github.com/plexusone/omnillm@v0.10.0\ngo mod tidy\n</code></pre>"},{"location":"releases/v0.10.0/#provider-default-timeouts","title":"Provider Default Timeouts","text":"<p>For reference, the default timeouts when no <code>Timeout</code> is configured:</p> Provider Default Timeout OpenAI 30 seconds Anthropic 30 seconds xAI 60 seconds Ollama 60 seconds Gemini (SDK default) <p>Note: These defaults may be too short for reasoning models. Set <code>Timeout: 300 * time.Second</code> for longer-running inference tasks.</p>"},{"location":"releases/v0.11.0/","title":"Release Notes - OmniLLM v0.11.0","text":"<p>Release Date: 2026-01-10 Base Version: v0.10.0</p>"},{"location":"releases/v0.11.0/#overview","title":"Overview","text":"<p>Version 0.11.0 is a major feature release that adds four key reliability and cost optimization features: Fallback Providers, Circuit Breaker, Token Estimation, and Response Caching. This release also includes extended sampling parameters for fine-grained control over model outputs.</p> <p>\u26a0\ufe0f Breaking Change: <code>ClientConfig</code> API refactored to use unified <code>Providers []ProviderConfig</code> slice. See Upgrade Guide for migration instructions.</p> <p>Highlights:</p> <ul> <li>Unified Provider Configuration: Cleaner API with <code>Providers</code> slice (index 0 = primary, 1+ = fallbacks)</li> <li>Fallback Providers: Automatic failover to backup providers when primary fails</li> <li>Circuit Breaker: Prevent cascading failures by temporarily skipping unhealthy providers</li> <li>Token Estimation: Pre-flight validation to avoid context window limit errors</li> <li>Response Caching: Reduce API costs by caching identical requests</li> <li>Extended Sampling Parameters: TopK, Seed, N, ResponseFormat, Logprobs support</li> </ul>"},{"location":"releases/v0.11.0/#new-features","title":"New Features","text":""},{"location":"releases/v0.11.0/#1-fallback-providers","title":"1. Fallback Providers","text":"<p>Automatic failover to backup providers when the primary provider fails with retryable errors (rate limits, server errors, network issues).</p> <pre><code>// Providers[0] is primary, Providers[1+] are fallbacks\nclient, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOpenAI, APIKey: \"openai-key\"},       // Primary\n        {Provider: omnillm.ProviderNameAnthropic, APIKey: \"anthropic-key\"}, // Fallback 1\n        {Provider: omnillm.ProviderNameGemini, APIKey: \"gemini-key\"},       // Fallback 2\n    },\n})\n\n// If OpenAI fails, automatically tries Anthropic, then Gemini\nresponse, err := client.CreateChatCompletion(ctx, request)\n</code></pre> <p>Key Features:</p> <ul> <li>Intelligent error classification (only retries on retryable errors)</li> <li>Auth errors (401/403) and invalid requests (400) do not trigger fallback</li> <li><code>FallbackError</code> type provides detailed attempt tracking</li> <li>Works with both sync and streaming APIs</li> </ul>"},{"location":"releases/v0.11.0/#2-circuit-breaker","title":"2. Circuit Breaker","text":"<p>Prevents cascading failures by temporarily skipping providers that are failing repeatedly.</p> <pre><code>client, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOpenAI, APIKey: \"openai-key\"},\n        {Provider: omnillm.ProviderNameAnthropic, APIKey: \"anthropic-key\"},\n    },\n    CircuitBreakerConfig: &amp;omnillm.CircuitBreakerConfig{\n        FailureThreshold: 5,               // Open after 5 consecutive failures\n        SuccessThreshold: 2,               // Close after 2 successes in half-open\n        Timeout:          30 * time.Second, // Wait before trying again\n    },\n})\n</code></pre> <p>Circuit States:</p> State Description Closed Normal operation, requests flow through Open Provider is failing, requests skip immediately Half-Open Testing if provider has recovered"},{"location":"releases/v0.11.0/#3-token-estimation","title":"3. Token Estimation","text":"<p>Pre-flight token counting to validate requests before sending to the API.</p> <pre><code>// Standalone estimation\nestimator := omnillm.NewTokenEstimator(omnillm.DefaultTokenEstimatorConfig())\ntokens, _ := estimator.EstimateTokens(\"gpt-4o\", messages)\nwindow := estimator.GetContextWindow(\"gpt-4o\") // 128000\n\n// Automatic validation in client\nclient, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOpenAI, APIKey: \"your-key\"},\n    },\n    TokenEstimator: omnillm.NewTokenEstimator(omnillm.DefaultTokenEstimatorConfig()),\n    ValidateTokens: true,\n})\n</code></pre> <p>Built-in Context Windows:</p> <ul> <li>40+ models supported (OpenAI, Anthropic, Gemini, X.AI, Ollama)</li> <li>Custom context windows via <code>CustomContextWindows</code> map</li> <li>Configurable characters-per-token ratio</li> </ul>"},{"location":"releases/v0.11.0/#4-response-caching","title":"4. Response Caching","text":"<p>Cache identical requests to reduce API costs with configurable TTL.</p> <pre><code>client, err := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOpenAI, APIKey: \"your-key\"},\n    },\n    Cache: kvsClient, // Redis, DynamoDB, etc.\n    CacheConfig: &amp;omnillm.CacheConfig{\n        TTL:       1 * time.Hour,\n        KeyPrefix: \"myapp:llm-cache\",\n    },\n})\n\n// Check if response was cached\nif resp.ProviderMetadata[\"cache_hit\"] == true {\n    // Response came from cache\n}\n</code></pre> <p>Cache Key Generation:</p> <ul> <li>SHA-256 hash of model, messages, and parameters</li> <li>Configurable inclusion of temperature and seed in cache key</li> <li>Model allowlist for selective caching</li> <li>Streaming requests skipped by default</li> </ul>"},{"location":"releases/v0.11.0/#5-extended-sampling-parameters","title":"5. Extended Sampling Parameters","text":"<p>New parameters for fine-grained control over model outputs:</p> Parameter Type Providers Description <code>TopK</code> <code>*int</code> Anthropic, Gemini, Ollama Top K token selection <code>Seed</code> <code>*int</code> OpenAI, X.AI, Ollama Reproducible outputs <code>N</code> <code>*int</code> OpenAI Number of completions <code>ResponseFormat</code> <code>*ResponseFormat</code> OpenAI, Gemini JSON mode <code>Logprobs</code> <code>*bool</code> OpenAI Return log probabilities <code>TopLogprobs</code> <code>*int</code> OpenAI Top logprobs count"},{"location":"releases/v0.11.0/#new-types","title":"New Types","text":""},{"location":"releases/v0.11.0/#error-classification","title":"Error Classification","text":"<pre><code>type ErrorCategory int\n\nconst (\n    ErrorCategoryUnknown ErrorCategory = iota\n    ErrorCategoryRetryable    // Rate limits, server errors, network errors\n    ErrorCategoryNonRetryable // Auth errors, invalid requests\n)\n\nfunc ClassifyError(err error) ErrorCategory\nfunc IsRetryableError(err error) bool\nfunc IsNonRetryableError(err error) bool\n</code></pre>"},{"location":"releases/v0.11.0/#token-types","title":"Token Types","text":"<pre><code>type TokenEstimator interface {\n    EstimateTokens(model string, messages []provider.Message) (int, error)\n    GetContextWindow(model string) int\n}\n\ntype TokenLimitError struct {\n    EstimatedTokens int\n    ContextWindow   int\n    AvailableTokens int\n    Model           string\n}\n</code></pre>"},{"location":"releases/v0.11.0/#fallback-types","title":"Fallback Types","text":"<pre><code>type FallbackError struct {\n    Attempts  []FallbackAttempt\n    LastError error\n}\n\ntype FallbackAttempt struct {\n    Provider string\n    Error    error\n    Duration time.Duration\n}\n</code></pre>"},{"location":"releases/v0.11.0/#updated-clientconfig","title":"Updated ClientConfig","text":"<pre><code>type ClientConfig struct {\n    // Provider configuration (BREAKING CHANGE in v0.11.0)\n    Providers []ProviderConfig  // Index 0 = primary, 1+ = fallbacks\n\n    // Circuit Breaker (optional)\n    CircuitBreakerConfig *CircuitBreakerConfig\n\n    // Memory\n    Memory       kvs.Client\n    MemoryConfig *MemoryConfig\n\n    // Observability\n    ObservabilityHook ObservabilityHook\n    Logger            *slog.Logger\n\n    // Token Estimation\n    TokenEstimator TokenEstimator\n    ValidateTokens bool\n\n    // Response Caching\n    Cache       kvs.Client\n    CacheConfig *CacheConfig\n}\n\ntype ProviderConfig struct {\n    Provider       ProviderName\n    APIKey         string\n    BaseURL        string\n    Region         string\n    Timeout        time.Duration\n    HTTPClient     *http.Client\n    Extra          map[string]any\n    CustomProvider provider.Provider  // For 3rd party providers\n}\n</code></pre>"},{"location":"releases/v0.11.0/#upgrade-guide","title":"Upgrade Guide","text":""},{"location":"releases/v0.11.0/#from-v0100","title":"From v0.10.0","text":"<p>\u26a0\ufe0f Breaking Change: The <code>ClientConfig</code> API has been refactored to use a unified <code>Providers</code> slice.</p> <pre><code>go get github.com/plexusone/omnillm@v0.11.0\ngo mod tidy\n</code></pre>"},{"location":"releases/v0.11.0/#migration-basic-client","title":"Migration: Basic Client","text":"<pre><code>// Before (v0.10.0)\nclient, _ := omnillm.NewClient(omnillm.ClientConfig{\n    Provider: omnillm.ProviderNameOpenAI,\n    APIKey:   apiKey,\n})\n\n// After (v0.11.0)\nclient, _ := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOpenAI, APIKey: apiKey},\n    },\n})\n</code></pre>"},{"location":"releases/v0.11.0/#migration-with-fallback-providers","title":"Migration: With Fallback Providers","text":"<pre><code>// Before (v0.10.0)\nclient, _ := omnillm.NewClient(omnillm.ClientConfig{\n    Provider: omnillm.ProviderNameOpenAI,\n    APIKey:   apiKey,\n    FallbackProviders: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameAnthropic, APIKey: anthropicKey},\n    },\n})\n\n// After (v0.11.0) - Providers[0] is primary, Providers[1+] are fallbacks\nclient, _ := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOpenAI, APIKey: apiKey},\n        {Provider: omnillm.ProviderNameAnthropic, APIKey: anthropicKey},\n    },\n})\n</code></pre>"},{"location":"releases/v0.11.0/#migration-custom-http-client-timeout","title":"Migration: Custom HTTP Client / Timeout","text":"<pre><code>// Before (v0.10.0)\nclient, _ := omnillm.NewClient(omnillm.ClientConfig{\n    Provider:   omnillm.ProviderNameOpenAI,\n    APIKey:     apiKey,\n    Timeout:    5 * time.Minute,\n    HTTPClient: customHTTPClient,\n})\n\n// After (v0.11.0) - Timeout/HTTPClient moved to ProviderConfig\nclient, _ := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {\n            Provider:   omnillm.ProviderNameOpenAI,\n            APIKey:     apiKey,\n            Timeout:    5 * time.Minute,\n            HTTPClient: customHTTPClient,\n        },\n    },\n})\n</code></pre>"},{"location":"releases/v0.11.0/#migration-custom-provider","title":"Migration: Custom Provider","text":"<pre><code>// Before (v0.10.0)\nclient, _ := omnillm.NewClient(omnillm.ClientConfig{\n    CustomProvider: myCustomProvider,\n})\n\n// After (v0.11.0) - CustomProvider moved to ProviderConfig\nclient, _ := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {CustomProvider: myCustomProvider},\n    },\n})\n</code></pre>"},{"location":"releases/v0.11.0/#enable-token-validation","title":"Enable Token Validation","text":"<pre><code>client, _ := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOpenAI, APIKey: apiKey},\n    },\n    TokenEstimator: omnillm.NewTokenEstimator(omnillm.DefaultTokenEstimatorConfig()),\n    ValidateTokens: true,\n})\n</code></pre>"},{"location":"releases/v0.11.0/#enable-response-caching","title":"Enable Response Caching","text":"<pre><code>client, _ := omnillm.NewClient(omnillm.ClientConfig{\n    Providers: []omnillm.ProviderConfig{\n        {Provider: omnillm.ProviderNameOpenAI, APIKey: apiKey},\n    },\n    Cache: kvsClient, // Your KVS implementation\n    CacheConfig: &amp;omnillm.CacheConfig{\n        TTL: 1 * time.Hour,\n    },\n})\n</code></pre>"},{"location":"releases/v0.11.0/#new-files","title":"New Files","text":"File Description <code>circuitbreaker.go</code> Circuit breaker implementation <code>circuitbreaker_test.go</code> Circuit breaker tests <code>fallback.go</code> Fallback provider wrapper <code>fallback_test.go</code> Fallback tests <code>tokens.go</code> Token estimation <code>tokens_test.go</code> Token estimation tests <code>cache.go</code> Response caching <code>cache_test.go</code> Cache tests"},{"location":"releases/v0.11.0/#test-coverage","title":"Test Coverage","text":"<ul> <li>Main package: 72.7% coverage</li> <li>New feature code: 78-95% coverage</li> <li>45+ unit tests</li> </ul>"},{"location":"releases/v0.11.0/#performance-considerations","title":"Performance Considerations","text":""},{"location":"releases/v0.11.0/#fallback-providers","title":"Fallback Providers","text":"<ul> <li>Fallback adds minimal latency when primary succeeds</li> <li>Circuit breaker prevents unnecessary attempts to failing providers</li> <li>Consider provider ordering by latency/cost</li> </ul>"},{"location":"releases/v0.11.0/#token-estimation","title":"Token Estimation","text":"<ul> <li>Character-based estimation is fast but approximate</li> <li>Actual token count may vary by 5-15%</li> <li>Use conservative estimates for critical applications</li> </ul>"},{"location":"releases/v0.11.0/#response-caching","title":"Response Caching","text":"<ul> <li>Cache lookups add ~1ms latency</li> <li>TTL should match your freshness requirements</li> <li>Consider memory/storage costs for cache backend</li> </ul>"},{"location":"releases/v0.11.0/#related-documentation","title":"Related Documentation","text":"<ul> <li>README - Full feature documentation</li> <li>CHANGELOG - Complete change history</li> <li>ROADMAP - Future plans</li> </ul>"},{"location":"releases/v0.12.0/","title":"v0.12.0","text":"<p>Release Date: 2026-02-22</p>"},{"location":"releases/v0.12.0/#highlights","title":"Highlights","text":"<ul> <li>Tool/function calling support for OpenAI provider</li> </ul>"},{"location":"releases/v0.12.0/#added","title":"Added","text":"<ul> <li><code>Tool</code>, <code>ToolSpec</code>, <code>ToolCall</code>, <code>ToolFunction</code> types for tool calling</li> <li><code>Tools</code> and <code>ToolChoice</code> fields in <code>ChatCompletionRequest</code></li> <li><code>ToolCallID</code> and <code>ToolCalls</code> fields in <code>Message</code></li> <li>Conversion between unified tool format and OpenAI format</li> </ul>"},{"location":"releases/v0.12.0/#tool-calling-example","title":"Tool Calling Example","text":"<pre><code>response, err := client.CreateChatCompletion(ctx, &amp;omnillm.ChatCompletionRequest{\n    Model: omnillm.ModelGPT4o,\n    Messages: []omnillm.Message{\n        {Role: omnillm.RoleUser, Content: \"What's the weather in Tokyo?\"},\n    },\n    Tools: []omnillm.Tool{\n        {\n            Type: \"function\",\n            Function: omnillm.ToolFunction{\n                Name:        \"get_weather\",\n                Description: \"Get current weather for a location\",\n                Parameters: map[string]any{\n                    \"type\": \"object\",\n                    \"properties\": map[string]any{\n                        \"location\": map[string]any{\n                            \"type\":        \"string\",\n                            \"description\": \"City name\",\n                        },\n                    },\n                    \"required\": []string{\"location\"},\n                },\n            },\n        },\n    },\n})\n\n// Check for tool calls in response\nif len(response.Choices[0].Message.ToolCalls) &gt; 0 {\n    // Execute tool and send result back\n}\n</code></pre>"},{"location":"releases/v0.12.0/#upgrade-notes","title":"Upgrade Notes","text":"<p>This is a backwards-compatible feature addition. No changes required for existing code.</p>"},{"location":"releases/v0.13.0/","title":"Release Notes: v0.13.0","text":"<p>Release Date: 2026-02-28</p>"},{"location":"releases/v0.13.0/#highlights","title":"Highlights","text":"<ul> <li>Organization Rename: GitHub organization renamed from <code>agentplexus</code> to <code>plexusone</code></li> </ul>"},{"location":"releases/v0.13.0/#breaking-changes","title":"Breaking Changes","text":"<p>This release contains a breaking change due to the organization rename:</p> Component Before After Go module <code>github.com/agentplexus/omnillm</code> <code>github.com/plexusone/omnillm</code>"},{"location":"releases/v0.13.0/#migration-guide","title":"Migration Guide","text":"<p>Update your import paths:</p> <pre><code>// Before\nimport \"github.com/agentplexus/omnillm\"\n\n// After\nimport \"github.com/plexusone/omnillm\"\n</code></pre> <p>Update your <code>go.mod</code>:</p> <pre><code># Remove old dependency\ngo mod edit -droprequire github.com/agentplexus/omnillm\n\n# Add new dependency\ngo get github.com/plexusone/omnillm@v0.13.0\n\n# Clean up\ngo mod tidy\n</code></pre>"},{"location":"releases/v0.13.0/#installation","title":"Installation","text":"<pre><code>go get github.com/plexusone/omnillm@v0.13.0\n</code></pre>"},{"location":"releases/v0.6.0/","title":"Release Notes - GoLLM v0.6.0","text":"<p>Release Date: 2025-12-14 Base Version: v0.5.1 (commit d8763ee)</p>"},{"location":"releases/v0.6.0/#overview","title":"\ud83c\udf89 Overview","text":"<p>Version 0.6.0 is a major feature release that adds X.AI Grok provider support, implements Anthropic streaming, introduces comprehensive testing infrastructure, and enhances the canonical response structure to preserve provider-specific metadata.</p> <p>Summary: - \u2728 New X.AI Grok provider with full streaming support - \u2728 Complete Anthropic streaming implementation - \ud83e\uddea Comprehensive test suite with 44 tests (unit + integration) - \ud83d\udcca Provider metadata preservation in responses - \ud83c\udfd7\ufe0f Mock infrastructure for testing - \ud83d\udcda Enhanced documentation and examples - \ud83d\udd27 Code quality improvements</p> <p>Statistics: - Files Changed: 22 files - Lines Added: 3,041 insertions - Lines Removed: 61 deletions - New Files: 12 files - Test Coverage: 44 automated tests</p>"},{"location":"releases/v0.6.0/#new-features","title":"\u2728 New Features","text":""},{"location":"releases/v0.6.0/#1-xai-grok-provider-support","title":"1. X.AI Grok Provider Support","text":"<p>Complete implementation of X.AI's Grok API with OpenAI-compatible interface.</p> <p>New Files: - <code>providers/xai/xai.go</code> - HTTP client with SSE streaming (204 lines) - <code>providers/xai/types.go</code> - Request/response types (68 lines) - <code>providers/xai/adapter.go</code> - Provider interface implementation (159 lines) - <code>providers/xai/integration_test.go</code> - Integration tests (181 lines) - <code>examples/xai/main.go</code> - Usage examples (161 lines)</p> <p>Supported Models:</p> <p>Grok 4.1 (Latest - November 2025): - <code>grok-4-1-fast-reasoning</code> - Best tool-calling model with 2M context window - <code>grok-4-1-fast-non-reasoning</code> - Instant responses with 2M context window</p> <p>Grok 4 (July 2025): - <code>grok-4-0709</code> - Flagship model with 256K context - <code>grok-4-fast-reasoning</code> - Fast reasoning with 2M context - <code>grok-4-fast-non-reasoning</code> - Fast non-reasoning with 2M context - <code>grok-code-fast-1</code> - Coding-optimized model with 256K context</p> <p>Grok 3: - <code>grok-3</code> - Grok 3 model - <code>grok-3-mini</code> - Smaller, faster variant</p> <p>Grok 2: - <code>grok-2-1212</code> - Grok 2 (December 2024) - <code>grok-2-vision-1212</code> - Grok 2 with vision capabilities</p> <p>Example Usage: <pre><code>client, err := gollm.NewClient(gollm.ClientConfig{\n    Provider: gollm.ProviderNameXAI,\n    APIKey:   os.Getenv(\"XAI_API_KEY\"),\n})\n</code></pre></p> <p>Constants Added: - <code>ProviderNameXAI</code> - Provider identifier - <code>EnvVarXAIAPIKey</code> - Environment variable constant - Grok 4.1 Models: <code>ModelGrok4_1FastReasoning</code>, <code>ModelGrok4_1FastNonReasoning</code> - Grok 4 Models: <code>ModelGrok4_0709</code>, <code>ModelGrok4FastReasoning</code>, <code>ModelGrok4FastNonReasoning</code>, <code>ModelGrokCodeFast1</code> - Grok 3 Models: <code>ModelGrok3</code>, <code>ModelGrok3Mini</code> - Grok 2 Models: <code>ModelGrok2_1212</code>, <code>ModelGrok2_Vision</code></p>"},{"location":"releases/v0.6.0/#2-anthropic-streaming-implementation","title":"2. Anthropic Streaming Implementation","text":"<p>Full native streaming support for Anthropic Claude using Server-Sent Events (SSE).</p> <p>Files Modified: - <code>providers/anthropic/anthropic.go</code> - Added SSE streaming client (+126 lines) - <code>providers/anthropic/adapter.go</code> - Streaming adapter implementation (+174 lines) - <code>providers/anthropic/types.go</code> - Streaming event types (+32 lines)</p> <p>New Streaming Types: - <code>StreamEvent</code> - SSE event container - <code>StreamDelta</code> - Delta content for streaming - <code>StreamMessage</code> - Message metadata - <code>StreamUsage</code> - Streaming usage information</p> <p>Example: <pre><code>stream, err := client.CreateChatCompletionStream(ctx, &amp;gollm.ChatCompletionRequest{\n    Model: gollm.ModelClaude3Sonnet,\n    Messages: []gollm.Message{\n        {Role: gollm.RoleUser, Content: \"Write a haiku\"},\n    },\n})\ndefer stream.Close()\n\nfor {\n    chunk, err := stream.Recv()\n    if err == io.EOF {\n        break\n    }\n    if len(chunk.Choices) &gt; 0 &amp;&amp; chunk.Choices[0].Delta != nil {\n        fmt.Print(chunk.Choices[0].Delta.Content)\n    }\n}\n</code></pre></p> <p>Example Added: - <code>examples/anthropic_streaming/main.go</code> - Complete streaming demonstrations (158 lines)</p>"},{"location":"releases/v0.6.0/#3-provider-metadata-preservation","title":"3. Provider Metadata Preservation","text":"<p>New <code>ProviderMetadata</code> field in response structures to preserve provider-specific information.</p> <p>Files Modified: - <code>provider/types.go</code> - Added <code>ProviderMetadata map[string]any</code> to:   - <code>ChatCompletionResponse</code> (line 74)   - <code>ChatCompletionChunk</code> (line 102)</p> <p>Anthropic Metadata Preserved: - <code>anthropic_type</code> - Response type - <code>anthropic_role</code> - Response role - <code>anthropic_content</code> - Full content array (preserves multi-block responses) - <code>anthropic_stop_reason</code> - Stop reason - <code>anthropic_event_type</code> - Streaming event types - <code>anthropic_delta</code> - Delta objects - <code>anthropic_usage</code> - Usage information</p> <p>Example Usage: <pre><code>response, err := client.CreateChatCompletion(ctx, req)\n\n// Access standard unified fields\ncontent := response.Choices[0].Message.Content\n\n// Access Anthropic-specific metadata\nif metadata := response.ProviderMetadata; metadata != nil {\n    if fullContent, ok := metadata[\"anthropic_content\"].([]anthropic.Content); ok {\n        // Access all content blocks, not just first\n        for _, block := range fullContent {\n            fmt.Printf(\"Block [%s]: %s\\n\", block.Type, block.Text)\n        }\n    }\n}\n</code></pre></p> <p>Benefits: - \u2705 Backward compatible - existing code continues working - \u2705 Provider-agnostic - any provider can add metadata - \u2705 No data loss - preserves full provider responses - \u2705 Optional - field omitted when empty</p>"},{"location":"releases/v0.6.0/#4-comprehensive-testing-infrastructure","title":"4. Comprehensive Testing Infrastructure","text":"<p>Complete test suite with unit tests, integration tests, and mock implementations.</p> <p>New Test Files: - <code>client_test.go</code> - Client and memory integration tests (441 lines) - <code>memory_test.go</code> - Conversation memory tests (314 lines) - <code>providers/anthropic/adapter_test.go</code> - Anthropic unit tests (270 lines) - <code>providers/anthropic/integration_test.go</code> - Anthropic API tests (302 lines) - <code>providers/openai/integration_test.go</code> - OpenAI API tests (181 lines) - <code>providers/xai/integration_test.go</code> - X.AI API tests (181 lines) - <code>testing/mock_kvs.go</code> - Mock KVS implementation (102 lines)</p> <p>Test Statistics: - Total Tests: 44 automated tests - Unit Tests: ~30 tests (run without API keys) - Integration Tests: ~14 tests (conditional on API keys)</p> <p>Test Categories:</p> <ol> <li>Client Tests (<code>client_test.go</code>)</li> <li>Client creation and configuration</li> <li>Chat completion (sync and streaming)</li> <li>Memory-aware completions</li> <li>Conversation management</li> <li> <p>Provider injection</p> </li> <li> <p>Memory Tests (<code>memory_test.go</code>)</p> </li> <li>Load/save conversations</li> <li>Append messages</li> <li>Max message limits</li> <li>System message preservation</li> <li>Metadata handling</li> <li> <p>Conversation deletion</p> </li> <li> <p>Provider Tests (Anthropic, OpenAI, X.AI)</p> </li> <li>Message conversion</li> <li>Request/response mapping</li> <li>Streaming adapters</li> <li>Error handling</li> <li>Real API integration (conditional)</li> </ol> <p>Integration Test Pattern: <pre><code>func TestAnthropicIntegration_Streaming(t *testing.T) {\n    apiKey := os.Getenv(\"ANTHROPIC_API_KEY\")\n    if apiKey == \"\" {\n        t.Skip(\"Skipping integration test: ANTHROPIC_API_KEY not set\")\n    }\n    // Test actual API calls...\n}\n</code></pre></p> <p>Running Tests: <pre><code># Run all unit tests (no API keys required)\ngo test ./... -short\n\n# Run with coverage\ngo test ./... -short -cover\n\n# Run integration tests (requires API keys)\nANTHROPIC_API_KEY=key go test ./providers/anthropic -v\nOPENAI_API_KEY=key go test ./providers/openai -v\nXAI_API_KEY=key go test ./providers/xai -v\n</code></pre></p> <p>Mock Infrastructure: - <code>testing/mock_kvs.go</code> - In-memory KVS for testing conversation memory without external dependencies - Mock providers for unit testing client logic</p>"},{"location":"releases/v0.6.0/#improvements","title":"\ud83d\udd27 Improvements","text":""},{"location":"releases/v0.6.0/#code-quality","title":"Code Quality","text":"<ol> <li>Type Modernization</li> <li>Replaced <code>interface{}</code> with <code>any</code> throughout codebase</li> <li>Updated in <code>provider/types.go</code>, <code>client.go</code>, <code>providers/anthropic/adapter.go</code></li> <li> <p>Improves readability and follows Go 1.18+ conventions</p> </li> <li> <p>Lint Fixes</p> </li> <li>Fixed all <code>golangci-lint</code> issues</li> <li>Removed ineffectual assignments</li> <li>Removed unused struct fields</li> <li>Fixed <code>gofmt</code> issues</li> <li> <p>Result: 0 lint issues</p> </li> <li> <p>Memory Function Improvements</p> </li> <li> <p>Updated <code>memory.go</code> with better type safety (18 lines modified)</p> </li> <li> <p>X.AI Grok 4 Model Support \u2b50 NEW</p> </li> <li>Added 6 new Grok 4/4.1 model constants</li> <li>Grok 4.1 Fast: Latest with 2M context window and tool calling</li> <li>Grok 4 Fast: 2M context with reasoning/non-reasoning modes</li> <li>Grok Code Fast: Optimized for coding tasks (256K context)</li> <li>Updated examples and tests to use <code>grok-4-1-fast-reasoning</code></li> </ol>"},{"location":"releases/v0.6.0/#documentation","title":"Documentation","text":"<ol> <li>README.md Updates (166 lines modified)</li> <li>Added X.AI provider documentation</li> <li>Updated architecture diagram with <code>xai/</code> provider</li> <li>Added comprehensive Testing section</li> <li>Updated model listings for all providers:<ul> <li>OpenAI: Added GPT-5, GPT-4.1 models</li> <li>Anthropic: Added Claude-Opus-4.1, Claude-Opus-4, Claude-Sonnet-4, Claude-3.7-Sonnet, Claude-3.5-Haiku</li> <li>Gemini: Updated to current models</li> </ul> </li> <li>Enhanced Contributing section with test requirements</li> <li> <p>Added integration test examples with X.AI</p> </li> <li> <p>Provider Support Table Updated <pre><code>| Provider   | Models                           | Features                          |\n|------------|----------------------------------|-----------------------------------|\n| OpenAI     | GPT-5, GPT-4.1, GPT-4o, ...     | Chat, Streaming, Functions        |\n| Anthropic  | Claude-Opus-4.1, Sonnet-4, ...  | Chat, Streaming, System messages  |\n| Gemini     | Gemini-2.5-Pro, 2.5-Flash, ...  | Chat, Streaming                   |\n| Bedrock    | Claude models, Titan models      | Chat, Multiple model families     |\n| X.AI       | Grok-4.1-Fast, Grok-4, Grok-4-Fast, Grok-Code-Fast, Grok-3, Grok-2 | Chat, Streaming, 2M context, Tool calling |\n| Ollama     | Llama 3, Mistral, Gemma, ...    | Chat, Streaming, Local inference  |\n</code></pre></p> </li> <li> <p>New Examples</p> </li> <li><code>examples/anthropic_streaming/main.go</code> - 3 Anthropic streaming scenarios</li> <li><code>examples/xai/main.go</code> - 3 X.AI usage examples</li> </ol>"},{"location":"releases/v0.6.0/#statistics","title":"\ud83d\udcca Statistics","text":""},{"location":"releases/v0.6.0/#code-additions-by-component","title":"Code Additions by Component","text":"Component Lines Added Description X.AI Provider 612 Complete provider implementation Anthropic Streaming 332 Native SSE streaming support Test Infrastructure 1,689 Unit + integration + mock tests Examples 319 Usage demonstrations Documentation 166 README and inline docs Total 3,041 Net additions to codebase"},{"location":"releases/v0.6.0/#file-distribution","title":"File Distribution","text":"<ul> <li>Provider Code: 7 files (612 lines for X.AI, 332 for Anthropic streaming)</li> <li>Tests: 6 files (1,689 lines)</li> <li>Examples: 2 files (319 lines)</li> <li>Core Changes: 5 files (provider/types.go, client.go, constants.go, providers.go, memory.go)</li> <li>Mock Infrastructure: 1 file (102 lines)</li> </ul>"},{"location":"releases/v0.6.0/#breaking-changes","title":"\ud83d\udd04 Breaking Changes","text":"<p>None - This release is fully backward compatible.</p> <p>All changes are additive: - New provider (X.AI) doesn't affect existing providers - <code>ProviderMetadata</code> field is optional (<code>omitempty</code>) - Existing APIs unchanged - Test additions don't affect runtime behavior</p>"},{"location":"releases/v0.6.0/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ol> <li>Anthropic Streaming</li> <li>Previously unimplemented streaming now fully functional</li> <li>Proper SSE parsing with event type handling</li> <li> <p>Correct message assembly from multiple events</p> </li> <li> <p>Lint Issues</p> </li> <li>Fixed ineffectual assignment in <code>anthropic/anthropic.go:189</code></li> <li> <p>Removed unused field <code>streamChunkIndex</code> in <code>client_test.go:20</code></p> </li> <li> <p>Model Updates</p> </li> <li>Updated X.AI examples and tests to use latest <code>grok-4-1-fast-reasoning</code> model</li> <li>Added support for Grok 4 and Grok 4.1 model families (6 new model constants)</li> <li>Added deprecation markers for <code>grok-beta</code> and <code>grok-vision-beta</code></li> </ol>"},{"location":"releases/v0.6.0/#migration-guide","title":"\ud83d\ude80 Migration Guide","text":""},{"location":"releases/v0.6.0/#upgrading-from-v051","title":"Upgrading from v0.5.1","text":"<p>No code changes required - simply update your dependency:</p> <pre><code>go get -u github.com/grokify/gollm@v0.6.0\n</code></pre>"},{"location":"releases/v0.6.0/#using-new-features","title":"Using New Features","text":"<p>1. Add X.AI Grok Support: <pre><code>client, err := gollm.NewClient(gollm.ClientConfig{\n    Provider: gollm.ProviderNameXAI,\n    APIKey:   os.Getenv(\"XAI_API_KEY\"),\n})\n</code></pre></p> <p>2. Use Anthropic Streaming: <pre><code>stream, err := client.CreateChatCompletionStream(ctx, &amp;gollm.ChatCompletionRequest{\n    Model: gollm.ModelClaude3Sonnet,\n    Messages: []gollm.Message{\n        {Role: gollm.RoleUser, Content: \"Hello!\"},\n    },\n})\n</code></pre></p> <p>3. Access Provider Metadata: <pre><code>response, err := client.CreateChatCompletion(ctx, req)\nif metadata := response.ProviderMetadata; metadata != nil {\n    // Access provider-specific data\n    fmt.Printf(\"Metadata: %+v\\n\", metadata)\n}\n</code></pre></p> <p>4. Run Tests: <pre><code># Unit tests only\ngo test ./... -short\n\n# Integration tests (requires API keys)\nexport ANTHROPIC_API_KEY=your-key\nexport OPENAI_API_KEY=your-key\nexport XAI_API_KEY=your-key\ngo test ./... -v\n</code></pre></p>"},{"location":"releases/v0.6.0/#commits-since-v051","title":"\ud83d\udcdd Commits Since v0.5.1","text":"<pre><code>b9ed829 style: `any`: update from `interface{}`\na509740 feat: `provider`: add `ChatCompletionChunk.ProviderMetadata`\n803e5bb docs: `README.md`: update\n0c438f0 chore(lint): `golangci-lint`: fix various\n65917c8 feat: `providers/xai`: add support for xAI\nf21d267 chore(lint): `golangci-lint`: fix `gofmt`\n6fd5654 chore(lint): `golangci-lint`: fix `gofmt`\n536d73c docs: `README.md`: update for Anthropic streaming\n6514343 tests: add\n5f430f5 feat: `providers/anthropic`: add streaming support\n</code></pre>"},{"location":"releases/v0.6.0/#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<p>This release represents a significant expansion of GoLLM's capabilities with: - Complete X.AI Grok provider integration - Production-ready Anthropic streaming - Enterprise-grade test coverage - Enhanced metadata preservation</p>"},{"location":"releases/v0.6.0/#resources","title":"\ud83d\udcda Resources","text":"<ul> <li>Documentation: README</li> <li>Examples: See <code>/examples</code> directory</li> <li>Tests: Run <code>go test ./... -v</code> to see all test scenarios</li> <li>Issues: GitHub Issues</li> </ul> <p>Full Changelog: v0.5.1...v0.6.0</p>"},{"location":"releases/v0.7.0/","title":"Release Notes - FluxLLM v0.7.0","text":"<p>Release Date: 2025-12-21 Base Version: v0.6.1</p>"},{"location":"releases/v0.7.0/#overview","title":"Overview","text":"<p>Version 0.7.0 is a major release that renames the module from <code>gollm</code> to <code>fluxllm</code> and introduces comprehensive observability features including hooks for tracing/logging/metrics, injectable structured logging via <code>slog</code>, and context-aware logging for request-scoped correlation.</p> <p>Summary: - Module Rename: <code>github.com/grokify/gollm</code> \u2192 <code>github.com/grokify/fluxllm</code> - Observability Hooks: New <code>ObservabilityHook</code> interface for non-invasive tracing, logging, and metrics - Injectable Logging: <code>*slog.Logger</code> support with null logger default - Context-Aware Logging: Request-scoped logging via <code>slogutil.ContextWithLogger</code> - Custom HTTP Client: Injectable <code>*http.Client</code> for retry, tracing, and custom transports - Retry with Backoff: Automatic retries for transient failures (rate limits, 5xx errors) via <code>retryhttp</code> - Call Correlation: Unique <code>CallID</code> in <code>LLMCallInfo</code> for correlating hook calls - Bug Fix: Memory-aware methods now properly invoke observability hooks</p>"},{"location":"releases/v0.7.0/#breaking-changes","title":"Breaking Changes","text":""},{"location":"releases/v0.7.0/#module-rename","title":"Module Rename","text":"<p>The module has been renamed from <code>gollm</code> to <code>fluxllm</code>:</p> <p>Before: <pre><code>import \"github.com/grokify/gollm\"\n\nclient, err := gollm.NewClient(gollm.ClientConfig{...})\n</code></pre></p> <p>After: <pre><code>import \"github.com/grokify/fluxllm\"\n\nclient, err := fluxllm.NewClient(fluxllm.ClientConfig{...})\n</code></pre></p> <p>Migration: 1. Update import paths: <code>github.com/grokify/gollm</code> \u2192 <code>github.com/grokify/fluxllm</code> 2. Update type prefixes: <code>gollm.</code> \u2192 <code>fluxllm.</code> 3. Update go.mod: <code>go get github.com/grokify/fluxllm@v0.7.0</code></p>"},{"location":"releases/v0.7.0/#new-features","title":"New Features","text":""},{"location":"releases/v0.7.0/#1-observability-hooks","title":"1. Observability Hooks","text":"<p>New <code>ObservabilityHook</code> interface allows external packages to observe LLM calls without modifying the core library. This enables integration with OpenTelemetry, Datadog, custom metrics systems, and more.</p> <p>New File: <code>observability.go</code></p> <p>Interface: <pre><code>type LLMCallInfo struct {\n    CallID       string    // Unique identifier for correlating BeforeRequest/AfterResponse\n    ProviderName string    // e.g., \"openai\", \"anthropic\"\n    StartTime    time.Time // When the call started\n}\n\ntype ObservabilityHook interface {\n    // BeforeRequest is called before each LLM call.\n    // Returns a new context for trace/span propagation.\n    BeforeRequest(ctx context.Context, info LLMCallInfo, req *provider.ChatCompletionRequest) context.Context\n\n    // AfterResponse is called after each LLM call completes (success or failure).\n    AfterResponse(ctx context.Context, info LLMCallInfo, req *provider.ChatCompletionRequest, resp *provider.ChatCompletionResponse, err error)\n\n    // WrapStream wraps a stream for observability of streaming responses.\n    WrapStream(ctx context.Context, info LLMCallInfo, req *provider.ChatCompletionRequest, stream provider.ChatCompletionStream) provider.ChatCompletionStream\n}\n</code></pre></p> <p>Usage: <pre><code>type LoggingHook struct{}\n\nfunc (h *LoggingHook) BeforeRequest(ctx context.Context, info fluxllm.LLMCallInfo, req *fluxllm.ChatCompletionRequest) context.Context {\n    log.Printf(\"[%s] LLM call started: provider=%s model=%s\", info.CallID, info.ProviderName, req.Model)\n    return ctx\n}\n\nfunc (h *LoggingHook) AfterResponse(ctx context.Context, info fluxllm.LLMCallInfo, req *fluxllm.ChatCompletionRequest, resp *fluxllm.ChatCompletionResponse, err error) {\n    duration := time.Since(info.StartTime)\n    log.Printf(\"[%s] LLM call completed: duration=%v\", info.CallID, duration)\n}\n\nfunc (h *LoggingHook) WrapStream(ctx context.Context, info fluxllm.LLMCallInfo, req *fluxllm.ChatCompletionRequest, stream fluxllm.ChatCompletionStream) fluxllm.ChatCompletionStream {\n    return stream\n}\n\n// Configure hook\nclient, err := fluxllm.NewClient(fluxllm.ClientConfig{\n    Provider:          fluxllm.ProviderNameOpenAI,\n    APIKey:            \"your-api-key\",\n    ObservabilityHook: &amp;LoggingHook{},\n})\n</code></pre></p> <p>Key Features: - Non-invasive: Add observability without modifying core library code - Provider agnostic: Works with all LLM providers - Context propagation: Pass trace context through the entire call chain - Streaming support: Wrap streams to observe streaming responses - Unique CallID: Correlate BeforeRequest/AfterResponse in concurrent scenarios</p>"},{"location":"releases/v0.7.0/#2-injectable-structured-logging","title":"2. Injectable Structured Logging","text":"<p>FluxLLM now supports injectable logging via Go's standard <code>log/slog</code> package.</p> <p>New Fields: - <code>ClientConfig.Logger</code> - Optional <code>*slog.Logger</code> for internal logging - <code>ChatClient.Logger()</code> - Accessor method to retrieve the logger</p> <p>Behavior: - If no logger is provided, a null logger is used (zero overhead, no output) - Logger is used for non-critical errors that shouldn't fail the main request</p> <p>Usage: <pre><code>import (\n    \"log/slog\"\n    \"os\"\n    \"github.com/grokify/fluxllm\"\n)\n\nlogger := slog.New(slog.NewJSONHandler(os.Stderr, &amp;slog.HandlerOptions{\n    Level: slog.LevelDebug,\n}))\n\nclient, err := fluxllm.NewClient(fluxllm.ClientConfig{\n    Provider: fluxllm.ProviderNameOpenAI,\n    APIKey:   \"your-api-key\",\n    Logger:   logger,\n})\n\n// Access logger if needed\nclient.Logger().Info(\"client initialized\")\n</code></pre></p>"},{"location":"releases/v0.7.0/#3-context-aware-logging","title":"3. Context-Aware Logging","text":"<p>Support for request-scoped logging via context, enabling trace IDs, user IDs, and other request-specific attributes to flow through all log output.</p> <p>New Functions (in <code>github.com/grokify/mogo/log/slogutil</code> v0.72.5): - <code>slogutil.ContextWithLogger(ctx, logger)</code> - Attach a logger to context - <code>slogutil.LoggerFromContext(ctx, fallback)</code> - Retrieve logger from context</p> <p>Usage: <pre><code>import (\n    \"log/slog\"\n    \"github.com/grokify/fluxllm\"\n    \"github.com/grokify/mogo/log/slogutil\"\n)\n\n// Create request-scoped logger with trace context\nreqLogger := slog.Default().With(\n    slog.String(\"trace_id\", traceID),\n    slog.String(\"user_id\", userID),\n    slog.String(\"request_id\", requestID),\n)\n\n// Attach to context\nctx = slogutil.ContextWithLogger(ctx, reqLogger)\n\n// All internal logging will now include trace_id, user_id, request_id\nresponse, err := client.CreateChatCompletionWithMemory(ctx, sessionID, req)\n</code></pre></p>"},{"location":"releases/v0.7.0/#4-custom-http-client-with-retry-support","title":"4. Custom HTTP Client with Retry Support","text":"<p>FluxLLM now supports injecting a custom <code>*http.Client</code>, enabling retry with exponential backoff for transient failures (rate limits, 5xx errors), custom transports for tracing/metrics, and other HTTP-level middleware.</p> <p>New Field: - <code>ClientConfig.HTTPClient</code> - Optional <code>*http.Client</code> with custom transport</p> <p>Usage with Retry Transport: <pre><code>import (\n    \"net/http\"\n    \"time\"\n\n    \"github.com/grokify/fluxllm\"\n    \"github.com/grokify/mogo/net/http/retryhttp\"\n)\n\n// Create retry transport with exponential backoff\nrt := retryhttp.NewWithOptions(\n    retryhttp.WithMaxRetries(5),\n    retryhttp.WithInitialBackoff(500 * time.Millisecond),\n    retryhttp.WithMaxBackoff(30 * time.Second),\n    retryhttp.WithOnRetry(func(attempt int, req *http.Request, resp *http.Response, err error, backoff time.Duration) {\n        log.Printf(\"Retry attempt %d, waiting %v\", attempt, backoff)\n    }),\n)\n\n// Create client with retry-enabled HTTP client\nclient, err := fluxllm.NewClient(fluxllm.ClientConfig{\n    Provider: fluxllm.ProviderNameOpenAI,\n    APIKey:   os.Getenv(\"OPENAI_API_KEY\"),\n    HTTPClient: &amp;http.Client{\n        Transport: rt,\n        Timeout:   2 * time.Minute, // Allow time for retries\n    },\n})\n</code></pre></p> <p>Retry Transport Features (via <code>github.com/grokify/mogo/net/http/retryhttp</code>): - Configurable max retries (default: 3) - Exponential backoff with jitter (prevents thundering herd) - Respects <code>Retry-After</code> headers from API responses - Default retryable status codes: 429 (rate limit), 500, 502, 503, 504 - Custom <code>ShouldRetry</code> function for advanced retry logic - <code>OnRetry</code> callback for logging/metrics</p> <p>Key Benefits: - Non-breaking: Existing code works unchanged (nil defaults to provider's default client) - Flexible: Works with any custom transport (retry, tracing, metrics, mocking) - Provider Support: Works with OpenAI, Anthropic, X.AI, and Ollama providers</p>"},{"location":"releases/v0.7.0/#bug-fixes","title":"Bug Fixes","text":""},{"location":"releases/v0.7.0/#memory-aware-methods-now-invoke-observability-hooks","title":"Memory-Aware Methods Now Invoke Observability Hooks","text":"<p>Issue: <code>CreateChatCompletionWithMemory</code> and <code>CreateChatCompletionStreamWithMemory</code> were calling the provider directly, bypassing the observability hook.</p> <p>Before (broken): <pre><code>response, err := c.provider.CreateChatCompletion(ctx, &amp;memoryReq)  // Hook not called!\n</code></pre></p> <p>After (fixed): <pre><code>response, err := c.CreateChatCompletion(ctx, &amp;memoryReq)  // Hook is called\n</code></pre></p> <p>This ensures all LLM calls are properly observed regardless of whether memory is used.</p>"},{"location":"releases/v0.7.0/#dependencies","title":"Dependencies","text":""},{"location":"releases/v0.7.0/#updated","title":"Updated","text":"<ul> <li><code>github.com/grokify/mogo</code> v0.72.4 \u2192 v0.72.5</li> <li>Added <code>slogutil.ContextWithLogger()</code> and <code>slogutil.LoggerFromContext()</code></li> </ul>"},{"location":"releases/v0.7.0/#files-changed","title":"Files Changed","text":"File Changes <code>client.go</code> Added <code>ObservabilityHook</code> field, <code>Logger</code> field, <code>HTTPClient</code> field, hook invocations, context-aware logging <code>observability.go</code> New file - <code>LLMCallInfo</code>, <code>ObservabilityHook</code> interface, <code>newCallID()</code> <code>providers.go</code> Updated factory functions to pass <code>HTTPClient</code> to provider constructors <code>providers/openai/*.go</code> Added <code>httpClient</code> parameter to <code>New()</code> and <code>NewProvider()</code> <code>providers/anthropic/*.go</code> Added <code>httpClient</code> parameter to <code>New()</code> and <code>NewProvider()</code> <code>providers/xai/*.go</code> Added <code>httpClient</code> parameter to <code>New()</code> and <code>NewProvider()</code> <code>providers/ollama/*.go</code> Added <code>httpClient</code> parameter to <code>New()</code> and <code>NewProvider()</code> <code>go.mod</code> Module rename, mogo upgrade to v0.72.5 <code>README.md</code> Renamed to FluxLLM, added Observability Hooks, Logging, Retry with Backoff sections"},{"location":"releases/v0.7.0/#migration-guide","title":"Migration Guide","text":""},{"location":"releases/v0.7.0/#upgrading-from-v06x","title":"Upgrading from v0.6.x","text":"<p>1. Update import paths: <pre><code># Using sed (macOS/Linux)\nfind . -name \"*.go\" -exec sed -i '' 's|github.com/grokify/gollm|github.com/grokify/fluxllm|g' {} +\nfind . -name \"*.go\" -exec sed -i '' 's|gollm\\.|fluxllm.|g' {} +\n</code></pre></p> <p>2. Update go.mod: <pre><code>go get github.com/grokify/fluxllm@v0.7.0\ngo mod tidy\n</code></pre></p> <p>3. (Optional) Add observability: <pre><code>client, err := fluxllm.NewClient(fluxllm.ClientConfig{\n    Provider:          fluxllm.ProviderNameOpenAI,\n    APIKey:            apiKey,\n    ObservabilityHook: &amp;YourHook{},  // New\n    Logger:            slog.Default(), // New\n})\n</code></pre></p>"},{"location":"releases/v0.7.0/#api-additions","title":"API Additions","text":""},{"location":"releases/v0.7.0/#new-types","title":"New Types","text":"<ul> <li><code>LLMCallInfo</code> - Metadata about LLM calls (CallID, ProviderName, StartTime)</li> <li><code>ObservabilityHook</code> - Interface for observing LLM calls</li> </ul>"},{"location":"releases/v0.7.0/#new-functions","title":"New Functions","text":"<ul> <li><code>newCallID()</code> - Generates unique call IDs (internal)</li> </ul>"},{"location":"releases/v0.7.0/#new-fields","title":"New Fields","text":"<ul> <li><code>ClientConfig.ObservabilityHook</code> - Hook for tracing/logging/metrics</li> <li><code>ClientConfig.Logger</code> - Injectable <code>*slog.Logger</code></li> <li><code>ClientConfig.HTTPClient</code> - Optional <code>*http.Client</code> for custom transports (retry, tracing, etc.)</li> <li><code>ChatClient.hook</code> - Stores the observability hook</li> <li><code>ChatClient.logger</code> - Stores the logger</li> <li><code>LLMCallInfo.CallID</code> - Unique identifier for call correlation</li> </ul>"},{"location":"releases/v0.7.0/#new-methods","title":"New Methods","text":"<ul> <li><code>ChatClient.Logger()</code> - Returns the client's logger</li> </ul>"},{"location":"releases/v0.7.0/#full-changelog","title":"Full Changelog","text":"<p>v0.6.1...v0.7.0</p> <ul> <li>feat: rename module from <code>gollm</code> to <code>fluxllm</code></li> <li>feat: add <code>ObservabilityHook</code> interface for tracing/logging/metrics</li> <li>feat: add <code>LLMCallInfo</code> with <code>CallID</code> for call correlation</li> <li>feat: add injectable <code>*slog.Logger</code> with null logger default</li> <li>feat: add context-aware logging via <code>slogutil.ContextWithLogger</code></li> <li>feat: add <code>ClientConfig.HTTPClient</code> for custom HTTP transports (retry, tracing, etc.)</li> <li>feat: add retry with backoff support via <code>mogo/net/http/retryhttp</code></li> <li>fix: memory-aware methods now properly invoke observability hooks</li> <li>docs: update README.md for FluxLLM rename</li> <li>docs: add Observability Hooks documentation</li> <li>docs: add Logging Configuration documentation</li> <li>docs: add Context-Aware Logging documentation</li> <li>docs: add Retry with Backoff documentation</li> <li>chore: upgrade <code>github.com/grokify/mogo</code> to v0.72.5</li> </ul>"},{"location":"releases/v0.7.0/#acknowledgments","title":"Acknowledgments","text":"<p>This release focuses on production-readiness with comprehensive observability support, enabling teams to integrate FluxLLM with their existing monitoring and tracing infrastructure.</p>"},{"location":"releases/v0.7.0/#resources","title":"Resources","text":"<ul> <li>Documentation: README</li> <li>Examples: See <code>/examples</code> directory</li> <li>Issues: GitHub Issues</li> </ul> <p>Full Changelog: v0.6.1...v0.7.0</p>"},{"location":"releases/v0.8.0/","title":"Release Notes - MetaLLM v0.8.0","text":"<p>Release Date: 2025-12-22 Base Version: v0.7.1</p>"},{"location":"releases/v0.8.0/#overview","title":"Overview","text":"<p>Version 0.8.0 renames the module from <code>fluxllm</code> to <code>metallm</code> for consistency with the <code>metaserp</code> naming convention and adds project direction documentation.</p> <p>Summary:</p> <ul> <li>Module Rename: <code>github.com/grokify/fluxllm</code> \u2192 <code>github.com/grokify/metallm</code></li> <li>Documentation: Added ROADMAP.md with project direction</li> </ul>"},{"location":"releases/v0.8.0/#breaking-changes","title":"Breaking Changes","text":""},{"location":"releases/v0.8.0/#module-rename","title":"Module Rename","text":"<p>The module has been renamed from <code>fluxllm</code> to <code>metallm</code> for consistency with <code>metaserp</code>:</p> <p>Before: <pre><code>import \"github.com/grokify/fluxllm\"\n\nclient, err := fluxllm.NewClient(fluxllm.ClientConfig{...})\n</code></pre></p> <p>After: <pre><code>import \"github.com/grokify/metallm\"\n\nclient, err := metallm.NewClient(metallm.ClientConfig{...})\n</code></pre></p> <p>Migration:</p> <ol> <li>Update import paths: <code>github.com/grokify/fluxllm</code> \u2192 <code>github.com/grokify/metallm</code></li> <li>Update type prefixes: <code>fluxllm.</code> \u2192 <code>metallm.</code></li> <li>Update go.mod: <code>go get github.com/grokify/metallm@v0.8.0</code></li> </ol>"},{"location":"releases/v0.8.0/#new-features","title":"New Features","text":""},{"location":"releases/v0.8.0/#roadmapmd","title":"ROADMAP.md","text":"<p>Added project roadmap documentation outlining the direction and planned features for the unified LLM SDK.</p>"},{"location":"releases/v0.8.0/#upgrade-guide","title":"Upgrade Guide","text":""},{"location":"releases/v0.8.0/#from-v071","title":"From v0.7.1","text":"<ol> <li>Update import paths from <code>fluxllm</code> to <code>metallm</code></li> <li>Update go.mod dependency</li> <li>Run <code>go mod tidy</code></li> </ol> <pre><code>go get github.com/grokify/metallm@v0.8.0\ngo mod tidy\n</code></pre>"},{"location":"releases/v0.9.0/","title":"Release Notes - OmniLLM v0.9.0","text":"<p>Release Date: 2025-12-27 Base Version: v0.8.0</p>"},{"location":"releases/v0.9.0/#overview","title":"Overview","text":"<p>Version 0.9.0 moves the module to the plexusone organization and renames it to <code>omnillm</code>, establishing the project's permanent home.</p> <p>Summary:</p> <ul> <li>Module Rename: <code>github.com/grokify/metallm</code> \u2192 <code>github.com/plexusone/omnillm</code></li> <li>Organization Move: Project now maintained under the plexusone organization</li> <li>Dependency Updates: All dependencies updated via <code>go mod tidy</code></li> </ul>"},{"location":"releases/v0.9.0/#breaking-changes","title":"Breaking Changes","text":""},{"location":"releases/v0.9.0/#module-rename-and-organization-move","title":"Module Rename and Organization Move","text":"<p>The module has been renamed and moved to a new organization:</p> <p>Before: <pre><code>import \"github.com/grokify/metallm\"\n\nclient, err := metallm.NewClient(metallm.ClientConfig{...})\n</code></pre></p> <p>After: <pre><code>import \"github.com/plexusone/omnillm\"\n\nclient, err := omnillm.NewClient(omnillm.ClientConfig{...})\n</code></pre></p> <p>Migration:</p> <ol> <li>Update import paths: <code>github.com/grokify/metallm</code> \u2192 <code>github.com/plexusone/omnillm</code></li> <li>Update type prefixes: <code>metallm.</code> \u2192 <code>omnillm.</code></li> <li>Update go.mod: <code>go get github.com/plexusone/omnillm@v0.9.0</code></li> </ol>"},{"location":"releases/v0.9.0/#improvements","title":"Improvements","text":""},{"location":"releases/v0.9.0/#dependency-updates","title":"Dependency Updates","text":"<p>All Go module dependencies have been updated to their latest stable versions.</p>"},{"location":"releases/v0.9.0/#upgrade-guide","title":"Upgrade Guide","text":""},{"location":"releases/v0.9.0/#from-v080","title":"From v0.8.0","text":"<ol> <li>Update import paths from <code>github.com/grokify/metallm</code> to <code>github.com/plexusone/omnillm</code></li> <li>Update type prefixes from <code>metallm.</code> to <code>omnillm.</code></li> <li>Update go.mod dependency</li> </ol> <pre><code>go get github.com/plexusone/omnillm@v0.9.0\ngo mod tidy\n</code></pre>"},{"location":"releases/v0.9.0/#notes","title":"Notes","text":"<ul> <li>This is the final module rename. The <code>github.com/plexusone/omnillm</code> path is the permanent home for this project.</li> <li>All previous module names (<code>gollm</code>, <code>fluxllm</code>, <code>metallm</code>) are deprecated.</li> </ul>"}]}