# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.1.0/),
this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html),
and this changelog is generated by [Structured Changelog](https://github.com/grokify/structured-changelog).

## [Unreleased]

## [0.11.0] - 2026-01-10

### Added

- `TopK` sampling parameter in `ChatCompletionRequest` (Anthropic, Gemini, Ollama)
- `Seed` parameter for reproducible outputs (OpenAI, X.AI, Ollama)
- `N` parameter for multiple completions (OpenAI)
- `ResponseFormat` type and field for JSON mode responses (OpenAI, Gemini)
- `Logprobs` and `TopLogprobs` parameters for log probability output (OpenAI)

### Changed

- All provider adapters updated to pass through newly supported parameters
- ROADMAP.md updated with extended sampling parameters marked as complete

## [0.10.0] - 2026-01-04

### Added

- Claude 4.5 model family: `claude-opus-4-5-20251101`, `claude-sonnet-4-5-20250929`, `claude-haiku-4-5-20251001`
- `ClientConfig.Timeout` field for configurable HTTP client timeout (recommended 300s for reasoning models)
- Marp presentation and GitHub Pages HTML documentation

### Changed

- HTTP client handling refactored with `getHTTPClient()` helper integrating both `HTTPClient` and `Timeout` configurations

## [0.9.0] - 2025-12-27

### Changed

- **BREAKING:** Module renamed from `github.com/grokify/metallm` to `github.com/agentplexus/omnillm`
- Dependencies updated via `go mod`

## [0.8.0] - 2025-12-22

### Added

- ROADMAP.md with project direction

### Changed

- **BREAKING:** Module renamed from `github.com/grokify/fluxllm` to `github.com/grokify/metallm` for consistency with `metaserp`

## [0.7.1] - 2025-12-21

### Changed

- **BREAKING:** Bedrock provider moved to standalone SDK at `github.com/grokify/fluxllm-bedrock`

## [0.7.0] - 2025-12-21

### Added

- `ObservabilityHook` interface for non-invasive tracing, logging, and metrics integration
- `LLMCallInfo` with unique `CallID` for correlating hook calls in concurrent scenarios
- Injectable `*slog.Logger` with null logger default for structured logging
- Context-aware logging via `slogutil.ContextWithLogger` for request-scoped correlation
- `ClientConfig.HTTPClient` for custom HTTP transports (retry, tracing, metrics)
- Retry with exponential backoff support via `mogo/net/http/retryhttp`

### Changed

- **BREAKING:** Module renamed from `github.com/grokify/gollm` to `github.com/grokify/fluxllm`

### Fixed

- Memory-aware methods (`CreateChatCompletionWithMemory`, `CreateChatCompletionStreamWithMemory`) now properly invoke observability hooks

## [0.6.1] - 2025-12-15

### Added

- Model constants for all providers in `models/` package

### Changed

- README.md updated for v0.6.0 Grok models documentation

## [0.6.0] - 2025-12-14

### Added

- X.AI Grok provider with full streaming support and Grok 4.1/4/3/2 model families
- Anthropic native streaming via Server-Sent Events (SSE)
- `ProviderMetadata` field in `ChatCompletionResponse` and `ChatCompletionChunk` for preserving provider-specific information
- Comprehensive test suite with 44 automated tests (unit + integration)
- Mock KVS infrastructure for testing conversation memory
- Anthropic streaming example in `examples/anthropic_streaming`
- X.AI usage examples in `examples/xai`

### Changed

- Replaced `interface{}` with `any` throughout codebase (Go 1.18+ convention)

### Fixed

- Anthropic streaming now fully functional with proper SSE parsing and event type handling
- Ineffectual assignment in `anthropic/anthropic.go`
- All `golangci-lint` issues resolved

## [0.5.1] - 2025-12-14

### Changed

- CI Go versions updated
- Dependencies updated: `actions/setup-go` 5→6, `github/codeql-action` 3→4, `golangci/golangci-lint-action` 8→9, `actions/checkout` 5→6
- Go module dependencies updated: `github.com/grokify/sogo` 0.12.1→0.12.7, `github.com/aws/aws-sdk-go-v2/config` 1.31.6→1.31.13

## [0.5.0] - 2025-09-07

### Added

- Google Gemini provider with chat and streaming support

### Changed

- README.md documentation formatting improvements

### Fixed

- Various linting issues resolved

## [0.4.0] - 2025-08-31

### Added

- Custom external provider support via `Provider` interface injection
- Custom provider example in `examples/custom_provider`

### Changed

- **BREAKING:** Major provider architecture refactor (3 phases): internal providers moved to provider-specific directories
- Provider constants moved to `constants.go`
- README.md updated with Ollama in architecture section

## [0.3.0] - 2025-08-31

### Added

- Ollama provider for local LLM inference
- Memory documentation
- Ollama example in `examples/providers_demo`

### Changed

- `examples/basic` refactored to eliminate code duplication
- Dependencies updated: AWS SDK Bedrock Runtime 1.37.1→1.38.0, AWS SDK Config 1.31.2→1.31.5, sogo 0.12.0→0.12.1

### Fixed

- Linting issues: EOF newline compliance

## [0.2.0] - 2025-08-26

### Added

- Conversation memory feature for stateful interactions
- CodeQL SAST security scanning in CI
- Model constants in dedicated file

### Changed

- Constants reorganized into `constants.go`
- README.md formatting improvements
- golangci-lint configuration updated for false positives

## [0.1.0] - 2025-08-26

### Changed

- README.md updated with correct `pkg.go.dev` link

## [0.0.1] - 2025-08-25

### Added

- Initial release with unified LLM SDK
- OpenAI provider with GPT model support
- Anthropic Claude provider
- AWS Bedrock provider
- Chat completions API with synchronous and streaming support
- CI/CD workflows: golangci-lint, dependabot, CodeQL
- MIT License

[unreleased]: https://github.com/agentplexus/omnillm/compare/v0.11.0...HEAD
[0.11.0]: https://github.com/agentplexus/omnillm/compare/v0.10.0...v0.11.0
[0.10.0]: https://github.com/agentplexus/omnillm/compare/v0.9.0...v0.10.0
[0.9.0]: https://github.com/agentplexus/omnillm/compare/v0.8.0...v0.9.0
[0.8.0]: https://github.com/agentplexus/omnillm/compare/v0.7.1...v0.8.0
[0.7.1]: https://github.com/agentplexus/omnillm/compare/v0.7.0...v0.7.1
[0.7.0]: https://github.com/agentplexus/omnillm/compare/v0.6.1...v0.7.0
[0.6.1]: https://github.com/agentplexus/omnillm/compare/v0.6.0...v0.6.1
[0.6.0]: https://github.com/agentplexus/omnillm/compare/v0.5.1...v0.6.0
[0.5.1]: https://github.com/agentplexus/omnillm/compare/v0.5.0...v0.5.1
[0.5.0]: https://github.com/agentplexus/omnillm/compare/v0.4.0...v0.5.0
[0.4.0]: https://github.com/agentplexus/omnillm/compare/v0.3.0...v0.4.0
[0.3.0]: https://github.com/agentplexus/omnillm/compare/v0.2.0...v0.3.0
[0.2.0]: https://github.com/agentplexus/omnillm/compare/v0.1.0...v0.2.0
[0.1.0]: https://github.com/agentplexus/omnillm/compare/v0.0.1...v0.1.0
[0.0.1]: https://github.com/agentplexus/omnillm/releases/tag/v0.0.1
