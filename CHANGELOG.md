# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.1.0/),
this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html),
and this changelog is generated by [Structured Changelog](https://github.com/grokify/structured-changelog).

## [Unreleased]

## [0.11.0] - 2026-01-10

### Highlights

- **Reliability**: Automatic failover with fallback providers and circuit breaker pattern ensure your app stays up even when providers fail
- **Cost Optimization**: Response caching reduces API costs by returning cached responses for identical requests
- **Error Prevention**: Pre-flight token estimation validates requests before sending, avoiding context window limit errors

### Added

- **Fallback Providers**: Automatic failover to backup providers when primary fails (retryable errors only) ([`6a9f4c9`](https://github.com/agentplexus/omnillm/commit/6a9f4c9))
- **Circuit Breaker**: Prevents cascading failures by temporarily skipping unhealthy providers ([`f5bd1e9`](https://github.com/agentplexus/omnillm/commit/f5bd1e9))
- **Token Estimation**: Pre-flight token counting to validate requests before sending to API ([`60573ba`](https://github.com/agentplexus/omnillm/commit/60573ba))
- **Response Caching**: Cache identical requests using KVS backend with configurable TTL ([`1dc3c79`](https://github.com/agentplexus/omnillm/commit/1dc3c79))
- `FallbackProviders` and `CircuitBreakerConfig` fields in `ClientConfig` ([`582c556`](https://github.com/agentplexus/omnillm/commit/582c556))
- `TokenEstimator` interface with built-in context windows for 40+ models ([`60573ba`](https://github.com/agentplexus/omnillm/commit/60573ba))
- `CacheManager` with SHA-256 based cache key generation ([`1dc3c79`](https://github.com/agentplexus/omnillm/commit/1dc3c79))
- `ErrorCategory` type with `ClassifyError()` and `IsRetryableError()` for error classification ([`9939ee9`](https://github.com/agentplexus/omnillm/commit/9939ee9))
- `FallbackError` type with detailed attempt tracking ([`6a9f4c9`](https://github.com/agentplexus/omnillm/commit/6a9f4c9))
- `TokenLimitError` for pre-flight validation failures ([`60573ba`](https://github.com/agentplexus/omnillm/commit/60573ba))
- `TopK` sampling parameter in `ChatCompletionRequest` (Anthropic, Gemini, Ollama) ([`e7dc833`](https://github.com/agentplexus/omnillm/commit/e7dc833))
- `Seed` parameter for reproducible outputs (OpenAI, X.AI, Ollama) ([`e7dc833`](https://github.com/agentplexus/omnillm/commit/e7dc833))
- `N` parameter for multiple completions (OpenAI) ([`e7dc833`](https://github.com/agentplexus/omnillm/commit/e7dc833))
- `ResponseFormat` type and field for JSON mode responses (OpenAI, Gemini) ([`e7dc833`](https://github.com/agentplexus/omnillm/commit/e7dc833))
- `Logprobs` and `TopLogprobs` parameters for log probability output (OpenAI) ([`e7dc833`](https://github.com/agentplexus/omnillm/commit/e7dc833))
- `ErrNoProviders` error for when `ClientConfig.Providers` is empty

### Changed

- **BREAKING:** `ClientConfig` refactored to use unified `Providers []ProviderConfig` slice (index 0 = primary, 1+ = fallbacks)
- **BREAKING:** Removed redundant `Provider`, `APIKey`, `BaseURL`, `Region`, `Timeout`, `HTTPClient`, `CustomProvider`, `Extra`, `FallbackProviders` fields from `ClientConfig`
- `ProviderConfig` now supports `CustomProvider` field for 3rd party provider injection
- Provider constructors refactored to accept `ProviderConfig` instead of `ClientConfig`
- `CreateChatCompletion` now supports optional token validation and response caching ([`582c556`](https://github.com/agentplexus/omnillm/commit/582c556))
- All provider adapters updated to pass through newly supported parameters ([`e7dc833`](https://github.com/agentplexus/omnillm/commit/e7dc833))

### Documentation

- README.md updated with Fallback Providers, Circuit Breaker, Token Estimation, and Response Caching documentation ([`3e22925`](https://github.com/agentplexus/omnillm/commit/3e22925))
- RELEASE_NOTES_v0.11.0.md added with comprehensive release notes ([`3e22925`](https://github.com/agentplexus/omnillm/commit/3e22925))

### Tests

- Circuit breaker tests added (`circuitbreaker_test.go`) ([`f5bd1e9`](https://github.com/agentplexus/omnillm/commit/f5bd1e9))
- Fallback provider tests added (`fallback_test.go`) ([`6a9f4c9`](https://github.com/agentplexus/omnillm/commit/6a9f4c9))
- Token estimation tests added (`tokens_test.go`) ([`60573ba`](https://github.com/agentplexus/omnillm/commit/60573ba))
- Response caching tests added (`cache_test.go`) ([`1dc3c79`](https://github.com/agentplexus/omnillm/commit/1dc3c79))

## [0.10.0] - 2026-01-04

### Added

- Claude 4.5 model family: `claude-opus-4-5-20251101`, `claude-sonnet-4-5-20250929`, `claude-haiku-4-5-20251001`
- `ClientConfig.Timeout` field for configurable HTTP client timeout (recommended 300s for reasoning models)
- Marp presentation and GitHub Pages HTML documentation

### Changed

- HTTP client handling refactored with `getHTTPClient()` helper integrating both `HTTPClient` and `Timeout` configurations

## [0.9.0] - 2025-12-27

### Changed

- **BREAKING:** Module renamed from `github.com/grokify/metallm` to `github.com/agentplexus/omnillm`
- Dependencies updated via `go mod`

## [0.8.0] - 2025-12-22

### Added

- ROADMAP.md with project direction

### Changed

- **BREAKING:** Module renamed from `github.com/grokify/fluxllm` to `github.com/grokify/metallm` for consistency with `metaserp`

## [0.7.1] - 2025-12-21

### Changed

- **BREAKING:** Bedrock provider moved to standalone SDK at `github.com/grokify/fluxllm-bedrock`

## [0.7.0] - 2025-12-21

### Added

- `ObservabilityHook` interface for non-invasive tracing, logging, and metrics integration
- `LLMCallInfo` with unique `CallID` for correlating hook calls in concurrent scenarios
- Injectable `*slog.Logger` with null logger default for structured logging
- Context-aware logging via `slogutil.ContextWithLogger` for request-scoped correlation
- `ClientConfig.HTTPClient` for custom HTTP transports (retry, tracing, metrics)
- Retry with exponential backoff support via `mogo/net/http/retryhttp`

### Changed

- **BREAKING:** Module renamed from `github.com/grokify/gollm` to `github.com/grokify/fluxllm`

### Fixed

- Memory-aware methods (`CreateChatCompletionWithMemory`, `CreateChatCompletionStreamWithMemory`) now properly invoke observability hooks

## [0.6.1] - 2025-12-15

### Added

- Model constants for all providers in `models/` package

### Changed

- README.md updated for v0.6.0 Grok models documentation

## [0.6.0] - 2025-12-14

### Added

- X.AI Grok provider with full streaming support and Grok 4.1/4/3/2 model families
- Anthropic native streaming via Server-Sent Events (SSE)
- `ProviderMetadata` field in `ChatCompletionResponse` and `ChatCompletionChunk` for preserving provider-specific information
- Comprehensive test suite with 44 automated tests (unit + integration)
- Mock KVS infrastructure for testing conversation memory
- Anthropic streaming example in `examples/anthropic_streaming`
- X.AI usage examples in `examples/xai`

### Changed

- Replaced `interface{}` with `any` throughout codebase (Go 1.18+ convention)

### Fixed

- Anthropic streaming now fully functional with proper SSE parsing and event type handling
- Ineffectual assignment in `anthropic/anthropic.go`
- All `golangci-lint` issues resolved

## [0.5.1] - 2025-12-14

### Changed

- CI Go versions updated
- Dependencies updated: `actions/setup-go` 5→6, `github/codeql-action` 3→4, `golangci/golangci-lint-action` 8→9, `actions/checkout` 5→6
- Go module dependencies updated: `github.com/grokify/sogo` 0.12.1→0.12.7, `github.com/aws/aws-sdk-go-v2/config` 1.31.6→1.31.13

## [0.5.0] - 2025-09-07

### Added

- Google Gemini provider with chat and streaming support

### Changed

- README.md documentation formatting improvements

### Fixed

- Various linting issues resolved

## [0.4.0] - 2025-08-31

### Added

- Custom external provider support via `Provider` interface injection
- Custom provider example in `examples/custom_provider`

### Changed

- **BREAKING:** Major provider architecture refactor (3 phases): internal providers moved to provider-specific directories
- Provider constants moved to `constants.go`
- README.md updated with Ollama in architecture section

## [0.3.0] - 2025-08-31

### Added

- Ollama provider for local LLM inference
- Memory documentation
- Ollama example in `examples/providers_demo`

### Changed

- `examples/basic` refactored to eliminate code duplication
- Dependencies updated: AWS SDK Bedrock Runtime 1.37.1→1.38.0, AWS SDK Config 1.31.2→1.31.5, sogo 0.12.0→0.12.1

### Fixed

- Linting issues: EOF newline compliance

## [0.2.0] - 2025-08-26

### Added

- Conversation memory feature for stateful interactions
- CodeQL SAST security scanning in CI
- Model constants in dedicated file

### Changed

- Constants reorganized into `constants.go`
- README.md formatting improvements
- golangci-lint configuration updated for false positives

## [0.1.0] - 2025-08-26

### Changed

- README.md updated with correct `pkg.go.dev` link

## [0.0.1] - 2025-08-25

### Added

- Initial release with unified LLM SDK
- OpenAI provider with GPT model support
- Anthropic Claude provider
- AWS Bedrock provider
- Chat completions API with synchronous and streaming support
- CI/CD workflows: golangci-lint, dependabot, CodeQL
- MIT License

[unreleased]: https://github.com/agentplexus/omnillm/compare/0.11.0...HEAD
[0.11.0]: https://github.com/agentplexus/omnillm/compare/0.10.0...0.11.0
[0.10.0]: https://github.com/agentplexus/omnillm/compare/0.9.0...0.10.0
[0.9.0]: https://github.com/agentplexus/omnillm/compare/0.8.0...0.9.0
[0.8.0]: https://github.com/agentplexus/omnillm/compare/0.7.1...0.8.0
[0.7.1]: https://github.com/agentplexus/omnillm/compare/0.7.0...0.7.1
[0.7.0]: https://github.com/agentplexus/omnillm/compare/0.6.1...0.7.0
[0.6.1]: https://github.com/agentplexus/omnillm/compare/0.6.0...0.6.1
[0.6.0]: https://github.com/agentplexus/omnillm/compare/0.5.1...0.6.0
[0.5.1]: https://github.com/agentplexus/omnillm/compare/0.5.0...0.5.1
[0.5.0]: https://github.com/agentplexus/omnillm/compare/0.4.0...0.5.0
[0.4.0]: https://github.com/agentplexus/omnillm/compare/0.3.0...0.4.0
[0.3.0]: https://github.com/agentplexus/omnillm/compare/0.2.0...0.3.0
[0.2.0]: https://github.com/agentplexus/omnillm/compare/0.1.0...0.2.0
[0.1.0]: https://github.com/agentplexus/omnillm/compare/0.0.1...0.1.0
[0.0.1]: https://github.com/agentplexus/omnillm/releases/tag/0.0.1
