# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.1.0/),
this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html),
and this changelog is generated by [Structured Changelog](https://github.com/grokify/structured-changelog).

## [Unreleased]

## [v0.11.0] - 2026-01-10

### Highlights

- **Reliability**: Automatic failover with fallback providers and circuit breaker pattern ensure your app stays up even when providers fail
- **Cost Optimization**: Response caching reduces API costs by returning cached responses for identical requests
- **Error Prevention**: Pre-flight token estimation validates requests before sending, avoiding context window limit errors

### Added

- **Fallback Providers**: Automatic failover to backup providers when primary fails (retryable errors only) ([`6a9f4c9`](https://github.com/agentplexus/omnillm/commit/6a9f4c9))
- **Circuit Breaker**: Prevents cascading failures by temporarily skipping unhealthy providers ([`f5bd1e9`](https://github.com/agentplexus/omnillm/commit/f5bd1e9))
- **Token Estimation**: Pre-flight token counting to validate requests before sending to API ([`60573ba`](https://github.com/agentplexus/omnillm/commit/60573ba))
- **Response Caching**: Cache identical requests using KVS backend with configurable TTL ([`1dc3c79`](https://github.com/agentplexus/omnillm/commit/1dc3c79))
- `FallbackProviders` and `CircuitBreakerConfig` fields in `ClientConfig` ([`582c556`](https://github.com/agentplexus/omnillm/commit/582c556))
- `TokenEstimator` interface with built-in context windows for 40+ models ([`60573ba`](https://github.com/agentplexus/omnillm/commit/60573ba))
- `CacheManager` with SHA-256 based cache key generation ([`1dc3c79`](https://github.com/agentplexus/omnillm/commit/1dc3c79))
- `ErrorCategory` type with `ClassifyError()` and `IsRetryableError()` for error classification ([`9939ee9`](https://github.com/agentplexus/omnillm/commit/9939ee9))
- `FallbackError` type with detailed attempt tracking ([`6a9f4c9`](https://github.com/agentplexus/omnillm/commit/6a9f4c9))
- `TokenLimitError` for pre-flight validation failures ([`60573ba`](https://github.com/agentplexus/omnillm/commit/60573ba))
- `TopK` sampling parameter in `ChatCompletionRequest` (Anthropic, Gemini, Ollama) ([`e7dc833`](https://github.com/agentplexus/omnillm/commit/e7dc833))
- `Seed` parameter for reproducible outputs (OpenAI, X.AI, Ollama) ([`e7dc833`](https://github.com/agentplexus/omnillm/commit/e7dc833))
- `N` parameter for multiple completions (OpenAI) ([`e7dc833`](https://github.com/agentplexus/omnillm/commit/e7dc833))
- `ResponseFormat` type and field for JSON mode responses (OpenAI, Gemini) ([`e7dc833`](https://github.com/agentplexus/omnillm/commit/e7dc833))
- `Logprobs` and `TopLogprobs` parameters for log probability output (OpenAI) ([`e7dc833`](https://github.com/agentplexus/omnillm/commit/e7dc833))
- `ErrNoProviders` error for when `ClientConfig.Providers` is empty ([`0d06bd1`](https://github.com/agentplexus/omnillm/commit/0d06bd1))

### Changed

- **BREAKING:** `ClientConfig` refactored to use unified `Providers []ProviderConfig` slice (index 0 = primary, 1+ = fallbacks) ([`0d06bd1`](https://github.com/agentplexus/omnillm/commit/0d06bd1))
- **BREAKING:** Removed redundant `Provider`, `APIKey`, `BaseURL`, `Region`, `Timeout`, `HTTPClient`, `CustomProvider`, `Extra`, `FallbackProviders` fields from `ClientConfig` ([`0d06bd1`](https://github.com/agentplexus/omnillm/commit/0d06bd1))
- `ProviderConfig` now supports `CustomProvider` field for 3rd party provider injection ([`0d06bd1`](https://github.com/agentplexus/omnillm/commit/0d06bd1))
- Provider constructors refactored to accept `ProviderConfig` instead of `ClientConfig` ([`0d06bd1`](https://github.com/agentplexus/omnillm/commit/0d06bd1))
- `CreateChatCompletion` now supports optional token validation and response caching ([`582c556`](https://github.com/agentplexus/omnillm/commit/582c556))
- All provider adapters updated to pass through newly supported parameters ([`e7dc833`](https://github.com/agentplexus/omnillm/commit/e7dc833))

### Documentation

- README.md updated with Fallback Providers, Circuit Breaker, Token Estimation, and Response Caching documentation ([`3e22925`](https://github.com/agentplexus/omnillm/commit/3e22925))
- RELEASE_NOTES_v0.11.0.md added with comprehensive release notes ([`3e22925`](https://github.com/agentplexus/omnillm/commit/3e22925))

### Tests

- Circuit breaker tests added (`circuitbreaker_test.go`) ([`f5bd1e9`](https://github.com/agentplexus/omnillm/commit/f5bd1e9))
- Fallback provider tests added (`fallback_test.go`) ([`6a9f4c9`](https://github.com/agentplexus/omnillm/commit/6a9f4c9))
- Token estimation tests added (`tokens_test.go`) ([`60573ba`](https://github.com/agentplexus/omnillm/commit/60573ba))
- Response caching tests added (`cache_test.go`) ([`1dc3c79`](https://github.com/agentplexus/omnillm/commit/1dc3c79))

## [v0.10.0] - 2026-01-04

### Added

- Claude 4.5 model family: `claude-opus-4-5-20251101`, `claude-sonnet-4-5-20250929`, `claude-haiku-4-5-20251001` ([`d0d5eb7`](https://github.com/agentplexus/omnillm/commit/d0d5eb7))
- `ClientConfig.Timeout` field for configurable HTTP client timeout (recommended 300s for reasoning models) ([`d0d5eb7`](https://github.com/agentplexus/omnillm/commit/d0d5eb7))
- Marp presentation and GitHub Pages HTML documentation ([`823effa`](https://github.com/agentplexus/omnillm/commit/823effa))

### Changed

- HTTP client handling refactored with `getHTTPClient()` helper integrating both `HTTPClient` and `Timeout` configurations ([`d0d5eb7`](https://github.com/agentplexus/omnillm/commit/d0d5eb7))

## [v0.9.0] - 2025-12-27

### Changed

- **BREAKING:** Module renamed from `github.com/grokify/metallm` to `github.com/agentplexus/omnillm` ([`89059fb`](https://github.com/agentplexus/omnillm/commit/89059fb))
- Dependencies updated via `go mod` ([`459d7b0`](https://github.com/agentplexus/omnillm/commit/459d7b0))

## [v0.8.0] - 2025-12-22

### Added

- ROADMAP.md with project direction ([`424b1f7`](https://github.com/agentplexus/omnillm/commit/424b1f7))

### Changed

- **BREAKING:** Module renamed from `github.com/grokify/fluxllm` to `github.com/grokify/metallm` for consistency with `metaserp` ([`803e617`](https://github.com/agentplexus/omnillm/commit/803e617))

## [v0.7.1] - 2025-12-21

### Changed

- **BREAKING:** Bedrock provider moved to standalone SDK at `github.com/grokify/fluxllm-bedrock` ([`46126de`](https://github.com/agentplexus/omnillm/commit/46126de))

## [v0.7.0] - 2025-12-21

### Added

- `ObservabilityHook` interface for non-invasive tracing, logging, and metrics integration ([`609de92`](https://github.com/agentplexus/omnillm/commit/609de92))
- `LLMCallInfo` with unique `CallID` for correlating hook calls in concurrent scenarios ([`609de92`](https://github.com/agentplexus/omnillm/commit/609de92))
- Injectable `*slog.Logger` with null logger default for structured logging ([`c72feb3`](https://github.com/agentplexus/omnillm/commit/c72feb3))
- Context-aware logging via `slogutil.ContextWithLogger` for request-scoped correlation ([`c72feb3`](https://github.com/agentplexus/omnillm/commit/c72feb3))
- `ClientConfig.HTTPClient` for custom HTTP transports (retry, tracing, metrics) ([`609de92`](https://github.com/agentplexus/omnillm/commit/609de92))
- Retry with exponential backoff support via `mogo/net/http/retryhttp` ([`609de92`](https://github.com/agentplexus/omnillm/commit/609de92))

### Changed

- **BREAKING:** Module renamed from `github.com/grokify/gollm` to `github.com/grokify/fluxllm` ([`f65e355`](https://github.com/agentplexus/omnillm/commit/f65e355))

### Fixed

- Memory-aware methods (`CreateChatCompletionWithMemory`, `CreateChatCompletionStreamWithMemory`) now properly invoke observability hooks ([`fdf42ac`](https://github.com/agentplexus/omnillm/commit/fdf42ac))

## [v0.6.1] - 2025-12-15

### Added

- Model constants for all providers in `models/` package ([`fe9d331`](https://github.com/agentplexus/omnillm/commit/fe9d331))

### Changed

- README.md updated for v0.6.0 Grok models documentation ([`dc68f9c`](https://github.com/agentplexus/omnillm/commit/dc68f9c))

## [v0.6.0] - 2025-12-14

### Added

- X.AI Grok provider with full streaming support and Grok 4.1/4/3/2 model families ([`65917c8`](https://github.com/agentplexus/omnillm/commit/65917c8))
- Anthropic native streaming via Server-Sent Events (SSE) ([`5f430f5`](https://github.com/agentplexus/omnillm/commit/5f430f5))
- `ProviderMetadata` field in `ChatCompletionResponse` and `ChatCompletionChunk` for preserving provider-specific information ([`a509740`](https://github.com/agentplexus/omnillm/commit/a509740))
- Comprehensive test suite with 44 automated tests (unit + integration) ([`6514343`](https://github.com/agentplexus/omnillm/commit/6514343))
- Mock KVS infrastructure for testing conversation memory ([`6514343`](https://github.com/agentplexus/omnillm/commit/6514343))
- Anthropic streaming example in `examples/anthropic_streaming` ([`5f430f5`](https://github.com/agentplexus/omnillm/commit/5f430f5))
- X.AI usage examples in `examples/xai` ([`65917c8`](https://github.com/agentplexus/omnillm/commit/65917c8))

### Changed

- Replaced `interface{}` with `any` throughout codebase (Go 1.18+ convention) ([`b9ed829`](https://github.com/agentplexus/omnillm/commit/b9ed829))

### Fixed

- Anthropic streaming now fully functional with proper SSE parsing and event type handling ([`5f430f5`](https://github.com/agentplexus/omnillm/commit/5f430f5))
- Ineffectual assignment in `anthropic/anthropic.go` ([`0c438f0`](https://github.com/agentplexus/omnillm/commit/0c438f0))
- All `golangci-lint` issues resolved ([`0c438f0`](https://github.com/agentplexus/omnillm/commit/0c438f0))

## [v0.5.1] - 2025-12-14

### Changed

- CI Go versions updated ([`d8763ee`](https://github.com/agentplexus/omnillm/commit/d8763ee))
- Dependencies updated: `actions/setup-go` 5→6, `github/codeql-action` 3→4, `golangci/golangci-lint-action` 8→9, `actions/checkout` 5→6 ([`e515829`](https://github.com/agentplexus/omnillm/commit/e515829))
- Go module dependencies updated: `github.com/grokify/sogo` 0.12.1→0.12.7, `github.com/aws/aws-sdk-go-v2/config` 1.31.6→1.31.13 ([`e515829`](https://github.com/agentplexus/omnillm/commit/e515829))

## [v0.5.0] - 2025-09-07

### Added

- Google Gemini provider with chat and streaming support ([`8aebbaf`](https://github.com/agentplexus/omnillm/commit/8aebbaf))

### Changed

- README.md documentation formatting improvements ([`6cf247c`](https://github.com/agentplexus/omnillm/commit/6cf247c))

### Fixed

- Various linting issues resolved ([`f0d0c78`](https://github.com/agentplexus/omnillm/commit/f0d0c78))

## [v0.4.0] - 2025-08-31

### Added

- Custom external provider support via `Provider` interface injection ([`2406f7b`](https://github.com/agentplexus/omnillm/commit/2406f7b))
- Custom provider example in `examples/custom_provider` ([`24c557d`](https://github.com/agentplexus/omnillm/commit/24c557d))

### Changed

- **BREAKING:** Major provider architecture refactor (3 phases): internal providers moved to provider-specific directories ([`1452c4f`](https://github.com/agentplexus/omnillm/commit/1452c4f))
- Provider constants moved to `constants.go` ([`5c9f78b`](https://github.com/agentplexus/omnillm/commit/5c9f78b))
- README.md updated with Ollama in architecture section ([`8cb1c8d`](https://github.com/agentplexus/omnillm/commit/8cb1c8d))

## [v0.3.0] - 2025-08-31

### Added

- Ollama provider for local LLM inference ([`3fb07e7`](https://github.com/agentplexus/omnillm/commit/3fb07e7))
- Memory documentation ([`ac26c1a`](https://github.com/agentplexus/omnillm/commit/ac26c1a))
- Ollama example in `examples/providers_demo` ([`3da55df`](https://github.com/agentplexus/omnillm/commit/3da55df))

### Changed

- `examples/basic` refactored to eliminate code duplication ([`28459c8`](https://github.com/agentplexus/omnillm/commit/28459c8))
- Dependencies updated: AWS SDK Bedrock Runtime 1.37.1→1.38.0, AWS SDK Config 1.31.2→1.31.5, sogo 0.12.0→0.12.1 ([`59083a6`](https://github.com/agentplexus/omnillm/commit/59083a6))

### Fixed

- Linting issues: EOF newline compliance ([`216d166`](https://github.com/agentplexus/omnillm/commit/216d166))

## [v0.2.0] - 2025-08-26

### Added

- Conversation memory feature for stateful interactions ([`e6cad19`](https://github.com/agentplexus/omnillm/commit/e6cad19))
- CodeQL SAST security scanning in CI ([`cc96214`](https://github.com/agentplexus/omnillm/commit/cc96214))
- Model constants in dedicated file ([`ba6c471`](https://github.com/agentplexus/omnillm/commit/ba6c471))

### Changed

- Constants reorganized into `constants.go` ([`ba6c471`](https://github.com/agentplexus/omnillm/commit/ba6c471))
- README.md formatting improvements ([`27d754f`](https://github.com/agentplexus/omnillm/commit/27d754f))
- golangci-lint configuration updated for false positives ([`4047541`](https://github.com/agentplexus/omnillm/commit/4047541))

## [v0.1.0] - 2025-08-26

### Changed

- README.md updated with correct `pkg.go.dev` link ([`b618d78`](https://github.com/agentplexus/omnillm/commit/b618d78))

## [v0.0.1] - 2025-08-25

### Added

- Initial release with unified LLM SDK ([`3f6f4fd`](https://github.com/agentplexus/omnillm/commit/3f6f4fd))
- OpenAI provider with GPT model support ([`3f6f4fd`](https://github.com/agentplexus/omnillm/commit/3f6f4fd))
- Anthropic Claude provider ([`3f6f4fd`](https://github.com/agentplexus/omnillm/commit/3f6f4fd))
- AWS Bedrock provider ([`3f6f4fd`](https://github.com/agentplexus/omnillm/commit/3f6f4fd))
- Chat completions API with synchronous and streaming support ([`3f6f4fd`](https://github.com/agentplexus/omnillm/commit/3f6f4fd))
- CI/CD workflows: golangci-lint, dependabot, CodeQL ([`5c67463`](https://github.com/agentplexus/omnillm/commit/5c67463))
- MIT License ([`075eafd`](https://github.com/agentplexus/omnillm/commit/075eafd))

[unreleased]: https://github.com/agentplexus/omnillm/compare/v0.11.0...HEAD
[v0.11.0]: https://github.com/agentplexus/omnillm/compare/v0.10.0...v0.11.0
[v0.10.0]: https://github.com/agentplexus/omnillm/compare/v0.9.0...v0.10.0
[v0.9.0]: https://github.com/agentplexus/omnillm/compare/v0.8.0...v0.9.0
[v0.8.0]: https://github.com/agentplexus/omnillm/compare/v0.7.1...v0.8.0
[v0.7.1]: https://github.com/agentplexus/omnillm/compare/v0.7.0...v0.7.1
[v0.7.0]: https://github.com/agentplexus/omnillm/compare/v0.6.1...v0.7.0
[v0.6.1]: https://github.com/agentplexus/omnillm/compare/v0.6.0...v0.6.1
[v0.6.0]: https://github.com/agentplexus/omnillm/compare/v0.5.1...v0.6.0
[v0.5.1]: https://github.com/agentplexus/omnillm/compare/v0.5.0...v0.5.1
[v0.5.0]: https://github.com/agentplexus/omnillm/compare/v0.4.0...v0.5.0
[v0.4.0]: https://github.com/agentplexus/omnillm/compare/v0.3.0...v0.4.0
[v0.3.0]: https://github.com/agentplexus/omnillm/compare/v0.2.0...v0.3.0
[v0.2.0]: https://github.com/agentplexus/omnillm/compare/v0.1.0...v0.2.0
[v0.1.0]: https://github.com/agentplexus/omnillm/compare/v0.0.1...v0.1.0
[v0.0.1]: https://github.com/agentplexus/omnillm/releases/tag/v0.0.1
